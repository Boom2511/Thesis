{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Quick Weight Optimization - FaceForensics++ Processed Data\n",
    "\n",
    "## üìä Dataset Structure:\n",
    "```\n",
    "processed_data_split/\n",
    "‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ original/      (Real)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Deepfakes/     (Fake)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ FaceSwap/      (Fake)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Face2Face/     (Fake)\n",
    "‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ original/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Deepfakes/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ FaceSwap/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Face2Face/\n",
    "‚îî‚îÄ‚îÄ test/              ‚Üê ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ\n",
    "    ‚îú‚îÄ‚îÄ original/\n",
    "    ‚îú‚îÄ‚îÄ Deepfakes/\n",
    "    ‚îú‚îÄ‚îÄ FaceSwap/\n",
    "    ‚îî‚îÄ‚îÄ Face2Face/\n",
    "```\n",
    "\n",
    "## üéØ ‡∏à‡∏∞‡∏ó‡∏≥:\n",
    "1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• 3 ‡∏ï‡∏±‡∏ß\n",
    "2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö test set\n",
    "3. ‡∏´‡∏≤ optimal weights\n",
    "4. ‡∏™‡∏£‡πâ‡∏≤‡∏á config ‡πÉ‡∏´‡∏°‡πà\n",
    "\n",
    "## ‚ö° Compute Units: ~10-15 units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\n",
    "!pip install -q torch torchvision timm pillow scikit-learn tqdm\n",
    "!pip install -q git+https://github.com/openai/CLIP.git\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Step 2: ‡πÇ‡∏´‡∏•‡∏î Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° path ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)\n",
    "BASE_PATH = '/content/drive/MyDrive/DeepfakeProject/processed_data_split'\n",
    "TEST_PATH = f'{BASE_PATH}/test'\n",
    "\n",
    "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏†‡∏≤‡∏û\n",
    "real_images = glob.glob(f'{TEST_PATH}/original/**/*.jpg', recursive=True) + \\\n",
    "              glob.glob(f'{TEST_PATH}/original/**/*.png', recursive=True)\n",
    "\n",
    "fake_images = []\n",
    "fake_types = ['Deepfakes', 'FaceSwap', 'Face2Face']\n",
    "fake_counts = {}\n",
    "\n",
    "for fake_type in fake_types:\n",
    "    imgs = glob.glob(f'{TEST_PATH}/{fake_type}/**/*.jpg', recursive=True) + \\\n",
    "           glob.glob(f'{TEST_PATH}/{fake_type}/**/*.png', recursive=True)\n",
    "    fake_counts[fake_type] = len(imgs)\n",
    "    fake_images.extend(imgs)\n",
    "\n",
    "print(\"üìä Test Dataset Summary:\")\n",
    "print(f\"  Real (original):   {len(real_images):4d} images\")\n",
    "print(f\"  Fake (total):      {len(fake_images):4d} images\")\n",
    "for fake_type, count in fake_counts.items():\n",
    "    print(f\"    - {fake_type:12s} {count:4d} images\")\n",
    "print(f\"  \" + \"=\"*40)\n",
    "print(f\"  Total:             {len(real_images) + len(fake_images):4d} images\")\n",
    "print(f\"  Balance:           1 : {len(fake_images)/len(real_images):.2f} (real:fake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á test dataset list\n",
    "test_data = []\n",
    "\n",
    "# Real images (label = 0)\n",
    "for img_path in real_images:\n",
    "    test_data.append({'path': img_path, 'label': 0, 'type': 'real'})\n",
    "\n",
    "# Fake images (label = 1)\n",
    "for fake_type in fake_types:\n",
    "    imgs = glob.glob(f'{TEST_PATH}/{fake_type}/**/*.jpg', recursive=True) + \\\n",
    "           glob.glob(f'{TEST_PATH}/{fake_type}/**/*.png', recursive=True)\n",
    "    for img_path in imgs:\n",
    "        test_data.append({'path': img_path, 'label': 1, 'type': fake_type.lower()})\n",
    "\n",
    "print(f\"‚úÖ Test dataset ready: {len(test_data)} images\")\n",
    "\n",
    "# Shuffle (optional)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(test_data)\n",
    "print(\"‚úÖ Dataset shuffled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î model weights (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Drive)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå model weights 3 ‡πÑ‡∏ü‡∏•‡πå:\")\n",
    "print(\"1. xception_best.pth\")\n",
    "print(\"2. f3net_best.pth\")\n",
    "print(\"3. effort_clip_L14_trainOn_FaceForensic.pth\")\n",
    "print(\"\\n‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô Drive ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏∏ path ‡πÉ‡∏ô cell ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\")\n",
    "\n",
    "# Uncomment ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è ‡∏Å‡∏≥‡∏´‡∏ô‡∏î path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á model weights\n",
    "# Option A: ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô Colab (‡πÉ‡∏ä‡πâ path ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)\n",
    "WEIGHTS_PATH = '/content'\n",
    "\n",
    "# Option B: ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô Drive (‡∏õ‡∏£‡∏±‡∏ö path ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏à‡∏£‡∏¥‡∏á)\n",
    "# WEIGHTS_PATH = '/content/drive/MyDrive/DeepfakeProject/model_weights'\n",
    "\n",
    "XCEPTION_PATH = f'{WEIGHTS_PATH}/xception_best.pth'\n",
    "F3NET_PATH = f'{WEIGHTS_PATH}/f3net_best.pth'\n",
    "EFFORT_PATH = f'{WEIGHTS_PATH}/effort_clip_L14_trainOn_FaceForensic.pth'\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà\n",
    "import os\n",
    "for name, path in [('Xception', XCEPTION_PATH), ('F3Net', F3NET_PATH), ('Effort', EFFORT_PATH)]:\n",
    "    if os.path.exists(path):\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"‚úÖ {name:10s} found ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name:10s} NOT FOUND: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÇ‡∏´‡∏•‡∏î model classes\n",
    "# ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å model classes ‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏°‡∏≤‡∏ß‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà\n",
    "\n",
    "import timm\n",
    "import clip\n",
    "\n",
    "# ========================================\n",
    "# 1. Xception Model\n",
    "# ========================================\n",
    "class XceptionModel:\n",
    "    def __init__(self, weights_path: str, device: torch.device):\n",
    "        self.device = device\n",
    "        self.model = self._load_model(weights_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _load_model(self, weights_path: str) -> nn.Module:\n",
    "        model = timm.create_model('xception', pretrained=False, num_classes=2)\n",
    "        checkpoint = torch.load(weights_path, map_location=self.device)\n",
    "        \n",
    "        if isinstance(checkpoint, dict):\n",
    "            if 'model' in checkpoint:\n",
    "                state_dict = checkpoint['model']\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        # Clean state dict\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            k = k.replace('module.', '').replace('model.', '')\n",
    "            if 'fc.' in k:\n",
    "                k = k.replace('fc.', 'last_linear.')\n",
    "            new_state_dict[k] = v\n",
    "        \n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, image_tensor: torch.Tensor):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        logits = self.model(image_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        real_prob = probs[0][0].item()\n",
    "        fake_prob = probs[0][1].item()\n",
    "        return fake_prob, real_prob\n",
    "\n",
    "# ========================================\n",
    "# 2. F3Net Model\n",
    "# ========================================\n",
    "class F3NetModel:\n",
    "    def __init__(self, weights_path: str, device: torch.device):\n",
    "        self.device = device\n",
    "        self.model = self._load_model(weights_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _load_model(self, weights_path: str) -> nn.Module:\n",
    "        # F3Net ‡πÉ‡∏ä‡πâ architecture ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Xception\n",
    "        model = timm.create_model('xception', pretrained=False, num_classes=2)\n",
    "        checkpoint = torch.load(weights_path, map_location=self.device)\n",
    "        \n",
    "        if isinstance(checkpoint, dict):\n",
    "            if 'model' in checkpoint:\n",
    "                state_dict = checkpoint['model']\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                state_dict = checkpoint['state_dict']\n",
    "            else:\n",
    "                state_dict = checkpoint\n",
    "        else:\n",
    "            state_dict = checkpoint\n",
    "        \n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            k = k.replace('module.', '').replace('model.', '')\n",
    "            if 'fc.' in k:\n",
    "                k = k.replace('fc.', 'last_linear.')\n",
    "            new_state_dict[k] = v\n",
    "        \n",
    "        model.load_state_dict(new_state_dict, strict=False)\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, image_tensor: torch.Tensor):\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        logits = self.model(image_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        real_prob = probs[0][0].item()\n",
    "        fake_prob = probs[0][1].item()\n",
    "        return fake_prob, real_prob\n",
    "\n",
    "# ========================================\n",
    "# 3. Effort-CLIP Model\n",
    "# ========================================\n",
    "class EffortModel:\n",
    "    def __init__(self, weights_path: str, device: torch.device):\n",
    "        self.device = device\n",
    "        self.model, self.preprocess = self._load_model(weights_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _load_model(self, weights_path: str):\n",
    "        # ‡πÇ‡∏´‡∏•‡∏î CLIP model\n",
    "        model, preprocess = clip.load(\"ViT-L/14\", device=self.device)\n",
    "        \n",
    "        # ‡πÇ‡∏´‡∏•‡∏î classifier head\n",
    "        checkpoint = torch.load(weights_path, map_location=self.device)\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á classifier\n",
    "        classifier = nn.Linear(768, 2).to(self.device)  # CLIP ViT-L/14 = 768 dim\n",
    "        \n",
    "        # ‡πÇ‡∏´‡∏•‡∏î weights ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ\n",
    "        if 'classifier' in checkpoint:\n",
    "            classifier.load_state_dict(checkpoint['classifier'])\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        return model, preprocess\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, image_tensor: torch.Tensor):\n",
    "        # CLIP forward\n",
    "        features = self.model.encode_image(image_tensor.to(self.device))\n",
    "        features = features.float()\n",
    "        \n",
    "        # Classifier\n",
    "        logits = self.classifier(features)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        real_prob = probs[0][0].item()\n",
    "        fake_prob = probs[0][1].item()\n",
    "        return fake_prob, real_prob\n",
    "\n",
    "print(\"‚úÖ Model classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• 3 ‡∏ï‡∏±‡∏ß\n",
    "print(\"üì• Loading models...\\n\")\n",
    "\n",
    "xception = XceptionModel(XCEPTION_PATH, device)\n",
    "print(\"‚úÖ Xception loaded\")\n",
    "\n",
    "f3net = F3NetModel(F3NET_PATH, device)\n",
    "print(\"‚úÖ F3Net loaded\")\n",
    "\n",
    "effort = EffortModel(EFFORT_PATH, device)\n",
    "print(\"‚úÖ Effort-CLIP loaded\")\n",
    "\n",
    "models = {\n",
    "    'xception': xception,\n",
    "    'f3net': f3net,\n",
    "    'effort': effort\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Total models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 4: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Transform ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, model_name):\n",
    "    \"\"\"‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\"\"\"\n",
    "    print(f\"\\nüîç Evaluating {model_name}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in tqdm(test_data, desc=f\"{model_name}\"):\n",
    "        try:\n",
    "            img = Image.open(item['path']).convert('RGB')\n",
    "            img_tensor = transform(img).unsqueeze(0)\n",
    "            \n",
    "            fake_prob, real_prob = model.predict(img_tensor)\n",
    "            \n",
    "            predictions.append(fake_prob)\n",
    "            labels.append(item['label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error: {item['path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "print(\"‚úÖ Evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Starting Model Evaluation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    predictions, labels = evaluate_model(model, test_data, model_name)\n",
    "    results[model_name] = {\n",
    "        'predictions': predictions,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics\n",
    "    pred_labels = (predictions > 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, pred_labels)\n",
    "    prec = precision_score(labels, pred_labels, zero_division=0)\n",
    "    rec = recall_score(labels, pred_labels, zero_division=0)\n",
    "    f1 = f1_score(labels, pred_labels, zero_division=0)\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    \n",
    "    results[model_name]['metrics'] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä {model_name.upper()} Performance:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  AUC:       {auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ Evaluation Complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: ‡∏´‡∏≤ Optimal Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(weights, results):\n",
    "    \"\"\"‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ensemble ‡∏î‡πâ‡∏ß‡∏¢ weights ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\"\"\"\n",
    "    w_xception, w_f3net, w_effort = weights\n",
    "    \n",
    "    ensemble_pred = (\n",
    "        results['xception']['predictions'] * w_xception +\n",
    "        results['f3net']['predictions'] * w_f3net +\n",
    "        results['effort']['predictions'] * w_effort\n",
    "    )\n",
    "    \n",
    "    labels = results['xception']['labels']\n",
    "    pred_labels = (ensemble_pred > 0.5).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(labels, pred_labels)\n",
    "    f1 = f1_score(labels, pred_labels, zero_division=0)\n",
    "    auc = roc_auc_score(labels, ensemble_pred)\n",
    "    \n",
    "    return {'accuracy': acc, 'f1': f1, 'auc': auc}\n",
    "\n",
    "print(\"‚úÖ Ensemble evaluation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç Searching for Optimal Weights\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "step = 0.05\n",
    "weight_range = np.arange(0.0, 1.0 + step, step)\n",
    "\n",
    "best_score = 0\n",
    "best_weights = None\n",
    "best_metrics = None\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Grid search with step={step}\")\n",
    "print(f\"   Total combinations: ~{len(weight_range)**2} (filtered)\\n\")\n",
    "\n",
    "for w1 in tqdm(weight_range, desc=\"Grid Search\"):\n",
    "    for w2 in weight_range:\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        \n",
    "        if w3 < 0 or w3 > 1.0 or abs(w1 + w2 + w3 - 1.0) > 0.01:\n",
    "            continue\n",
    "        \n",
    "        weights = (w1, w2, w3)\n",
    "        metrics = evaluate_ensemble(weights, results)\n",
    "        score = metrics['f1']  # ‡πÉ‡∏ä‡πâ F1 score\n",
    "        \n",
    "        all_results.append({\n",
    "            'weights': weights,\n",
    "            'metrics': metrics,\n",
    "            'score': score\n",
    "        })\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_weights = weights\n",
    "            best_metrics = metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ BEST ENSEMBLE CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìä Optimal Weights:\")\n",
    "print(f\"  Xception:    {best_weights[0]:.3f} ({best_weights[0]*100:.1f}%)\")\n",
    "print(f\"  F3Net:       {best_weights[1]:.3f} ({best_weights[1]*100:.1f}%)\")\n",
    "print(f\"  Effort-CLIP: {best_weights[2]:.3f} ({best_weights[2]*100:.1f}%)\")\n",
    "print(f\"\\nüìà Performance:\")\n",
    "print(f\"  Accuracy: {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"  F1 Score: {best_metrics['f1']:.4f} ({best_metrics['f1']*100:.2f}%)\")\n",
    "print(f\"  AUC:      {best_metrics['auc']:.4f} ({best_metrics['auc']*100:.2f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 6: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Individual vs Ensemble\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models_names = ['Xception', 'F3Net', 'Effort-CLIP', 'Ensemble\\n(Optimized)']\n",
    "accuracies = [\n",
    "    results['xception']['metrics']['accuracy'],\n",
    "    results['f3net']['metrics']['accuracy'],\n",
    "    results['effort']['metrics']['accuracy'],\n",
    "    best_metrics['accuracy']\n",
    "]\n",
    "f1_scores = [\n",
    "    results['xception']['metrics']['f1'],\n",
    "    results['f3net']['metrics']['f1'],\n",
    "    results['effort']['metrics']['f1'],\n",
    "    best_metrics['f1']\n",
    "]\n",
    "aucs = [\n",
    "    results['xception']['metrics']['auc'],\n",
    "    results['f3net']['metrics']['auc'],\n",
    "    results['effort']['metrics']['auc'],\n",
    "    best_metrics['auc']\n",
    "]\n",
    "\n",
    "x = np.arange(len(models_names))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, accuracies, width, label='Accuracy', color='#3498db')\n",
    "bars2 = ax.bar(x, f1_scores, width, label='F1 Score', color='#e74c3c')\n",
    "bars3 = ax.bar(x + width, aucs, width, label='AUC', color='#2ecc71')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Individual Models vs Optimized Ensemble', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_names)\n",
    "ax.legend()\n",
    "ax.set_ylim([0.8, 1.0])  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Top 10 Weight Configurations\n",
    "top_10 = sorted(all_results, key=lambda x: x['score'], reverse=True)[:10]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "config_labels = [f\"Config {i+1}\" for i in range(10)]\n",
    "x_pos = np.arange(len(config_labels))\n",
    "\n",
    "# Extract weights\n",
    "xception_weights = [c['weights'][0] for c in top_10]\n",
    "f3net_weights = [c['weights'][1] for c in top_10]\n",
    "effort_weights = [c['weights'][2] for c in top_10]\n",
    "f1_scores_top = [c['metrics']['f1'] for c in top_10]\n",
    "\n",
    "width = 0.6\n",
    "p1 = ax.bar(x_pos, xception_weights, width, label='Xception', color='#3498db')\n",
    "p2 = ax.bar(x_pos, f3net_weights, width, bottom=xception_weights, label='F3Net', color='#e74c3c')\n",
    "p3 = ax.bar(x_pos, effort_weights, width, \n",
    "            bottom=np.array(xception_weights) + np.array(f3net_weights),\n",
    "            label='Effort-CLIP', color='#2ecc71')\n",
    "\n",
    "# Add F1 scores on top\n",
    "for i, f1 in enumerate(f1_scores_top):\n",
    "    ax.text(i, 1.02, f'{f1:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Weight Distribution', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Configuration (sorted by F1 score)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 10 Weight Configurations', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(config_labels, rotation=45)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.15])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top10_configurations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: top10_configurations.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset': {\n",
    "        'name': 'FaceForensics++ (processed)',\n",
    "        'split': 'test',\n",
    "        'total_images': len(test_data),\n",
    "        'real_images': len(real_images),\n",
    "        'fake_images': len(fake_images),\n",
    "        'fake_breakdown': fake_counts\n",
    "    },\n",
    "    'individual_models': {\n",
    "        'xception': results['xception']['metrics'],\n",
    "        'f3net': results['f3net']['metrics'],\n",
    "        'effort': results['effort']['metrics']\n",
    "    },\n",
    "    'best_ensemble': {\n",
    "        'weights': {\n",
    "            'xception': float(best_weights[0]),\n",
    "            'f3net': float(best_weights[1]),\n",
    "            'effort_clip': float(best_weights[2])\n",
    "        },\n",
    "        'metrics': best_metrics\n",
    "    },\n",
    "    'top_10_configurations': [\n",
    "        {\n",
    "            'rank': i+1,\n",
    "            'weights': {\n",
    "                'xception': float(r['weights'][0]),\n",
    "                'f3net': float(r['weights'][1]),\n",
    "                'effort_clip': float(r['weights'][2])\n",
    "            },\n",
    "            'f1_score': float(r['score']),\n",
    "            'accuracy': float(r['metrics']['accuracy']),\n",
    "            'auc': float(r['metrics']['auc'])\n",
    "        } for i, r in enumerate(sorted(all_results, key=lambda x: x['score'], reverse=True)[:10])\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å JSON\n",
    "with open('weight_optimization_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Report saved: weight_optimization_report.json\")\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á Top 5\n",
    "print(\"\\nüìä Top 5 Configurations:\")\n",
    "print(\"=\"*60)\n",
    "for i, config in enumerate(report['top_10_configurations'][:5], 1):\n",
    "    print(f\"\\n{i}. F1={config['f1_score']:.4f}, Acc={config['accuracy']:.4f}, AUC={config['auc']:.4f}\")\n",
    "    print(f\"   Xception: {config['weights']['xception']:.3f}, \"\n",
    "          f\"F3Net: {config['weights']['f3net']:.3f}, \"\n",
    "          f\"Effort: {config['weights']['effort_clip']:.3f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á config.json ‡πÉ‡∏´‡∏°‡πà\n",
    "new_config = {\n",
    "  \"models\": {\n",
    "    \"xception\": {\n",
    "      \"name\": \"xception\",\n",
    "      \"path\": \"app/models/weights/xception_best.pth\",\n",
    "      \"description\": \"Fast and reliable baseline\",\n",
    "      \"weight\": round(best_weights[0], 2),\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"efficientnet_b4\": {\n",
    "      \"name\": \"tf_efficientnet_b4\",\n",
    "      \"path\": \"app/models/weights/effnb4_best.pth\",\n",
    "      \"description\": \"Balanced performance (DISABLED: incompatible checkpoint)\",\n",
    "      \"weight\": 0.0,\n",
    "      \"enabled\": False\n",
    "    },\n",
    "    \"f3net\": {\n",
    "      \"name\": \"f3net\",\n",
    "      \"path\": \"app/models/weights/f3net_best.pth\",\n",
    "      \"description\": \"Frequency-aware network with spatial attention\",\n",
    "      \"weight\": round(best_weights[1], 2),\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"effort\": {\n",
    "      \"name\": \"effort_clip\",\n",
    "      \"path\": \"app/models/weights/effort_clip_L14_trainOn_FaceForensic.pth\",\n",
    "      \"description\": \"CLIP-based multimodal detection\",\n",
    "      \"weight\": round(best_weights[2], 2),\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  },\n",
    "  \"ensemble\": {\n",
    "    \"method\": \"weighted_average\",\n",
    "    \"threshold\": 0.5,\n",
    "    \"min_models\": 2\n",
    "  },\n",
    "  \"device\": \"cuda\",\n",
    "  \"face_detection\": {\n",
    "    \"min_confidence\": 0.85,\n",
    "    \"min_face_size\": 40\n",
    "  },\n",
    "  \"inference\": {\n",
    "    \"batch_size\": 1,\n",
    "    \"generate_gradcam\": False\n",
    "  }\n",
    "}\n",
    "\n",
    "with open('config_optimized.json', 'w') as f:\n",
    "    json.dump(new_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Config saved: config_optimized.json\")\n",
    "print(\"\\nüìã ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏õ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà: backend/app/config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading results...\\n\")\n",
    "\n",
    "files.download('weight_optimization_report.json')\n",
    "files.download('config_optimized.json')\n",
    "files.download('model_comparison.png')\n",
    "files.download('top10_configurations.png')\n",
    "\n",
    "print(\"\\n‚úÖ All files downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ ‡∏™‡∏£‡∏∏‡∏õ\n",
    "\n",
    "### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:\n",
    "1. **Optimal Weights** ‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ö‡∏ô FaceForensics++ test set\n",
    "2. **Performance Report** ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô (accuracy, F1, AUC)\n",
    "3. **Config File** ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "4. **Visualizations** ‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°\n",
    "5. **Top 10 Configurations** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö\n",
    "\n",
    "### üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:\n",
    "- **Individual Models:** ‡∏î‡∏π‡∏à‡∏≤‡∏Å output ‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô\n",
    "- **Ensemble (Optimized):** \n",
    "  - Weights: Xception: X.XX, F3Net: X.XX, Effort: X.XX\n",
    "  - Accuracy: X.XXXX\n",
    "  - F1 Score: X.XXXX\n",
    "  - AUC: X.XXXX\n",
    "\n",
    "### üöÄ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:\n",
    "1. Download `config_optimized.json`\n",
    "2. ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `backend/app/config.json`\n",
    "3. Restart backend server\n",
    "4. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö real-world images!\n",
    "\n",
    "### üí° Tips:\n",
    "- ‡∏ñ‡πâ‡∏≤ ensemble ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (< 1%) ‚Üí weights ‡πÄ‡∏î‡∏¥‡∏°‡∏Å‡πá‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ\n",
    "- ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å ‚Üí ‡∏•‡∏≠‡∏á‡∏õ‡∏¥‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏±‡πâ‡∏ô\n",
    "- ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö val set ‡πÅ‡∏•‡∏∞ cross-validation ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "\n",
    "**‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
