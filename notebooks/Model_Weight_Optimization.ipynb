{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Ensemble Model Weight Optimization\n",
    "## ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Weight ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:**\n",
    "- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏• (Xception, F3Net, Effort-CLIP)\n",
    "- ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ weight ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "- ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Compute Units (‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 5-10 units)\n",
    "\n",
    "**Dataset ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**\n",
    "- FaceForensics++ (test set)\n",
    "- Celeb-DF (test set)\n",
    "- ‡∏´‡∏£‡∏∑‡∏≠ custom dataset ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\n",
    "!pip install torch torchvision timm pillow scikit-learn matplotlib seaborn tqdm\n",
    "!pip install facenet-pytorch\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ git clone\n",
    "# !git clone <your-repo-url>\n",
    "\n",
    "# ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå model weights ‡πÅ‡∏ö‡∏ö manual\n",
    "from google.colab import files\n",
    "print(\"üìÅ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå model weights (3 ‡πÑ‡∏ü‡∏•‡πå):\")\n",
    "print(\"1. xception_best.pth\")\n",
    "print(\"2. f3net_best.pth\")\n",
    "print(\"3. effort_clip_L14_trainOn_FaceForensic.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå\n",
    "sys.path.insert(0, '/content/deepfake-detection/backend/app')  # ‡∏õ‡∏£‡∏±‡∏ö path ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏≤‡∏á\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå\n",
    "from models.xception_model import XceptionModel\n",
    "from models.f3net_model import F3NetModel\n",
    "from models.effort_model import EffortModel\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "print(\"üì• Loading models...\")\n",
    "\n",
    "xception = XceptionModel('models/weights/xception_best.pth', device)\n",
    "print(\"‚úÖ Xception loaded\")\n",
    "\n",
    "f3net = F3NetModel('models/weights/f3net_best.pth', device)\n",
    "print(\"‚úÖ F3Net loaded\")\n",
    "\n",
    "effort = EffortModel('models/weights/effort_clip_L14_trainOn_FaceForensic.pth', device)\n",
    "print(\"‚úÖ Effort-CLIP loaded\")\n",
    "\n",
    "models = {\n",
    "    'xception': xception,\n",
    "    'f3net': f3net,\n",
    "    'effort': effort\n",
    "}\n",
    "\n",
    "print(f\"\\nüéØ Total models loaded: {len(models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: ‡πÉ‡∏ä‡πâ dataset ‡∏à‡∏≤‡∏Å Drive\n",
    "# ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ folder structure:\n",
    "# test_data/\n",
    "#   ‚îú‚îÄ‚îÄ real/\n",
    "#   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg\n",
    "#   ‚îÇ   ‚îú‚îÄ‚îÄ img2.jpg\n",
    "#   ‚îî‚îÄ‚îÄ fake/\n",
    "#       ‚îú‚îÄ‚îÄ img1.jpg\n",
    "#       ‚îú‚îÄ‚îÄ img2.jpg\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# ‡∏õ‡∏£‡∏±‡∏ö path ‡∏ï‡∏≤‡∏° dataset ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "TEST_DATA_PATH = '/content/drive/MyDrive/test_data'  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Å‡πá‡∏ö\n",
    "\n",
    "# ‡∏´‡∏≤ path ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "real_images = glob.glob(f'{TEST_DATA_PATH}/real/*.jpg') + glob.glob(f'{TEST_DATA_PATH}/real/*.png')\n",
    "fake_images = glob.glob(f'{TEST_DATA_PATH}/fake/*.jpg') + glob.glob(f'{TEST_DATA_PATH}/fake/*.png')\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"  Real images: {len(real_images)}\")\n",
    "print(f\"  Fake images: {len(fake_images)}\")\n",
    "print(f\"  Total: {len(real_images) + len(fake_images)}\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á dataset list\n",
    "test_data = []\n",
    "for img_path in real_images:\n",
    "    test_data.append({'path': img_path, 'label': 0})  # 0 = REAL\n",
    "for img_path in fake_images:\n",
    "    test_data.append({'path': img_path, 'label': 1})  # 1 = FAKE\n",
    "\n",
    "print(f\"\\n‚úÖ Test dataset ready: {len(test_data)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Face detector (optional - ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ crop face)\n",
    "from facenet_pytorch import MTCNN\n",
    "face_detector = MTCNN(keep_all=False, device='cpu', post_process=False, min_face_size=40)\n",
    "print(\"‚úÖ Preprocessing ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_model(model, image_tensor):\n",
    "    \"\"\"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\"\"\"\n",
    "    with torch.no_grad():\n",
    "        fake_prob, real_prob = model.predict(image_tensor)\n",
    "    return fake_prob, real_prob\n",
    "\n",
    "def evaluate_model(model, test_data, model_name):\n",
    "    \"\"\"‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\"\"\"\n",
    "    print(f\"\\nüîç Evaluating {model_name}...\")\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in tqdm(test_data, desc=f\"{model_name}\"):\n",
    "        try:\n",
    "            # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞ preprocess\n",
    "            img = Image.open(item['path']).convert('RGB')\n",
    "            \n",
    "            # Optional: crop face\n",
    "            # boxes, _ = face_detector.detect(img)\n",
    "            # if boxes is not None:\n",
    "            #     x1, y1, x2, y2 = map(int, boxes[0])\n",
    "            #     img = img.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
    "            fake_prob, real_prob = predict_single_model(model, img_tensor)\n",
    "            \n",
    "            predictions.append(fake_prob)\n",
    "            labels.append(item['label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error processing {item['path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(predictions), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    predictions, labels = evaluate_model(model, test_data, model_name)\n",
    "    results[model_name] = {\n",
    "        'predictions': predictions,\n",
    "        'labels': labels\n",
    "    }\n",
    "    \n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    pred_labels = (predictions > 0.5).astype(int)\n",
    "    acc = accuracy_score(labels, pred_labels)\n",
    "    prec = precision_score(labels, pred_labels)\n",
    "    rec = recall_score(labels, pred_labels)\n",
    "    f1 = f1_score(labels, pred_labels)\n",
    "    \n",
    "    results[model_name]['metrics'] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Performance:\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Weight ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def evaluate_ensemble(weights, results):\n",
    "    \"\"\"‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô ensemble ‡∏î‡πâ‡∏ß‡∏¢ weight ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î\"\"\"\n",
    "    w_xception, w_f3net, w_effort = weights\n",
    "    \n",
    "    # ‡∏£‡∏ß‡∏° predictions ‡∏à‡∏≤‡∏Å 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "    ensemble_pred = (\n",
    "        results['xception']['predictions'] * w_xception +\n",
    "        results['f3net']['predictions'] * w_f3net +\n",
    "        results['effort']['predictions'] * w_effort\n",
    "    )\n",
    "    \n",
    "    labels = results['xception']['labels']\n",
    "    pred_labels = (ensemble_pred > 0.5).astype(int)\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "    \n",
    "    acc = accuracy_score(labels, pred_labels)\n",
    "    f1 = f1_score(labels, pred_labels)\n",
    "    auc = roc_auc_score(labels, ensemble_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# Grid search ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö weights\n",
    "print(\"üîç Searching for optimal weights...\\n\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î weights ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ (‡∏£‡∏ß‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô 1.0)\n",
    "step = 0.05  # ‡∏Ç‡∏±‡πâ‡∏ô 0.05 (5%)\n",
    "weight_range = np.arange(0.0, 1.0 + step, step)\n",
    "\n",
    "best_score = 0\n",
    "best_weights = None\n",
    "best_metrics = None\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for w1 in weight_range:\n",
    "    for w2 in weight_range:\n",
    "        w3 = 1.0 - w1 - w2\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ w3 ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "        if w3 < 0 or w3 > 1.0 or abs(w1 + w2 + w3 - 1.0) > 0.01:\n",
    "            continue\n",
    "        \n",
    "        weights = (w1, w2, w3)\n",
    "        metrics = evaluate_ensemble(weights, results)\n",
    "        \n",
    "        # ‡πÉ‡∏ä‡πâ F1 score ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô (‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å accuracy/auc)\n",
    "        score = metrics['f1']\n",
    "        \n",
    "        all_results.append({\n",
    "            'weights': weights,\n",
    "            'metrics': metrics,\n",
    "            'score': score\n",
    "        })\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_weights = weights\n",
    "            best_metrics = metrics\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ BEST ENSEMBLE CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìä Optimal Weights:\")\n",
    "print(f\"  Xception:    {best_weights[0]:.3f}\")\n",
    "print(f\"  F3Net:       {best_weights[1]:.3f}\")\n",
    "print(f\"  Effort-CLIP: {best_weights[2]:.3f}\")\n",
    "print(f\"\\nüìà Performance:\")\n",
    "print(f\"  Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {best_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC:      {best_metrics['auc']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "model_names = ['Xception', 'F3Net', 'Effort-CLIP']\n",
    "metrics_names = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "for idx, (model_name, model_key) in enumerate(zip(model_names, ['xception', 'f3net', 'effort'])):\n",
    "    metrics = results[model_key]['metrics']\n",
    "    values = [metrics[m] for m in metrics_names]\n",
    "    \n",
    "    axes[idx].bar(metrics_names, values, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'])\n",
    "    axes[idx].set_title(f'{model_name} Performance', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('individual_model_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Weight optimization heatmap (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏£‡∏Å)\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á grid ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö visualization\n",
    "weight_grid = {}\n",
    "for result in all_results:\n",
    "    w1, w2, w3 = result['weights']\n",
    "    key = (round(w1, 2), round(w2, 2))\n",
    "    if key not in weight_grid:\n",
    "        weight_grid[key] = result['metrics']['f1']\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á heatmap data\n",
    "w1_vals = sorted(list(set([k[0] for k in weight_grid.keys()])))\n",
    "w2_vals = sorted(list(set([k[1] for k in weight_grid.keys()])))\n",
    "\n",
    "heatmap_data = np.zeros((len(w2_vals), len(w1_vals)))\n",
    "for i, w2 in enumerate(w2_vals):\n",
    "    for j, w1 in enumerate(w1_vals):\n",
    "        key = (w1, w2)\n",
    "        if key in weight_grid:\n",
    "            heatmap_data[i, j] = weight_grid[key]\n",
    "        else:\n",
    "            heatmap_data[i, j] = np.nan\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, xticklabels=[f'{w:.2f}' for w in w1_vals], \n",
    "            yticklabels=[f'{w:.2f}' for w in w2_vals],\n",
    "            cmap='RdYlGn', annot=False, cbar_kws={'label': 'F1 Score'})\n",
    "plt.xlabel('Xception Weight', fontsize=12)\n",
    "plt.ylabel('F3Net Weight', fontsize=12)\n",
    "plt.title('Ensemble Performance Heatmap\\n(Effort-CLIP weight = 1 - Xception - F3Net)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "\n",
    "# Mark best weights\n",
    "best_x = w1_vals.index(round(best_weights[0], 2))\n",
    "best_y = w2_vals.index(round(best_weights[1], 2))\n",
    "plt.scatter([best_x], [best_y], color='red', s=200, marker='*', \n",
    "            edgecolors='white', linewidths=2, label='Best Weights')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('weight_optimization_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Individual vs Ensemble\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "models_comparison = ['Xception', 'F3Net', 'Effort-CLIP', 'Ensemble\\n(Best)']\n",
    "accuracies = [\n",
    "    results['xception']['metrics']['accuracy'],\n",
    "    results['f3net']['metrics']['accuracy'],\n",
    "    results['effort']['metrics']['accuracy'],\n",
    "    best_metrics['accuracy']\n",
    "]\n",
    "f1_scores = [\n",
    "    results['xception']['metrics']['f1'],\n",
    "    results['f3net']['metrics']['f1'],\n",
    "    results['effort']['metrics']['f1'],\n",
    "    best_metrics['f1']\n",
    "]\n",
    "\n",
    "x = np.arange(len(models_comparison))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db')\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='#e74c3c')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Individual Models vs Ensemble Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_comparison)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('individual_vs_ensemble.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô\n",
    "report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_size': len(test_data),\n",
    "    'individual_models': {\n",
    "        'xception': results['xception']['metrics'],\n",
    "        'f3net': results['f3net']['metrics'],\n",
    "        'effort': results['effort']['metrics']\n",
    "    },\n",
    "    'best_ensemble': {\n",
    "        'weights': {\n",
    "            'xception': float(best_weights[0]),\n",
    "            'f3net': float(best_weights[1]),\n",
    "            'effort_clip': float(best_weights[2])\n",
    "        },\n",
    "        'metrics': best_metrics\n",
    "    },\n",
    "    'top_10_configurations': sorted(\n",
    "        [{\n",
    "            'weights': {\n",
    "                'xception': float(r['weights'][0]),\n",
    "                'f3net': float(r['weights'][1]),\n",
    "                'effort_clip': float(r['weights'][2])\n",
    "            },\n",
    "            'f1_score': float(r['score'])\n",
    "        } for r in all_results],\n",
    "        key=lambda x: x['f1_score'],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "}\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSON\n",
    "with open('weight_optimization_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Report saved to: weight_optimization_report.json\")\n",
    "print(\"\\nüìä Top 10 Configurations:\")\n",
    "for i, config in enumerate(report['top_10_configurations'][:5], 1):\n",
    "    print(f\"\\n{i}. F1={config['f1_score']:.4f}\")\n",
    "    print(f\"   Xception: {config['weights']['xception']:.3f}\")\n",
    "    print(f\"   F3Net:    {config['weights']['f3net']:.3f}\")\n",
    "    print(f\"   Effort:   {config['weights']['effort_clip']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù ‡∏™‡∏£‡πâ‡∏≤‡∏á config.json ‡πÉ‡∏´‡∏°‡πà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á config ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "new_config = {\n",
    "  \"models\": {\n",
    "    \"xception\": {\n",
    "      \"name\": \"xception\",\n",
    "      \"path\": \"app/models/weights/xception_best.pth\",\n",
    "      \"description\": \"Fast and reliable baseline\",\n",
    "      \"weight\": round(best_weights[0], 2),\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"efficientnet_b4\": {\n",
    "      \"name\": \"tf_efficientnet_b4\",\n",
    "      \"path\": \"app/models/weights/effnb4_best.pth\",\n",
    "      \"description\": \"Balanced performance (DISABLED: incompatible checkpoint format)\",\n",
    "      \"weight\": 0.0,\n",
    "      \"enabled\": False\n",
    "    },\n",
    "    \"f3net\": {\n",
    "      \"name\": \"f3net\",\n",
    "      \"path\": \"app/models/weights/f3net_best.pth\",\n",
    "      \"description\": \"Frequency-aware network with spatial attention\",\n",
    "      \"weight\": round(best_weights[1], 2),\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"effort\": {\n",
    "      \"name\": \"effort_clip\",\n",
    "      \"path\": \"app/models/weights/effort_clip_L14_trainOn_FaceForensic.pth\",\n",
    "      \"description\": \"CLIP-based multimodal detection\",\n",
    "      \"weight\": round(best_weights[2], 2),\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  },\n",
    "  \"ensemble\": {\n",
    "    \"method\": \"weighted_average\",\n",
    "    \"threshold\": 0.5,\n",
    "    \"min_models\": 2\n",
    "  },\n",
    "  \"device\": \"cuda\",\n",
    "  \"face_detection\": {\n",
    "    \"min_confidence\": 0.85,\n",
    "    \"min_face_size\": 40\n",
    "  },\n",
    "  \"inference\": {\n",
    "    \"batch_size\": 1,\n",
    "    \"generate_gradcam\": False\n",
    "  }\n",
    "}\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å config ‡πÉ‡∏´‡∏°‡πà\n",
    "with open('config_optimized.json', 'w') as f:\n",
    "    json.dump(new_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Optimized config saved to: config_optimized.json\")\n",
    "print(\"\\nüìã ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏õ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà: backend/app/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\n",
    "\n",
    "### ‚úÖ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:\n",
    "1. **‡∏Ñ‡πà‡∏≤ weight ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "2. **‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö** ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "3. **config.json ‡πÉ‡∏´‡∏°‡πà** ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "4. **‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î** ‡πÉ‡∏ô JSON format\n",
    "\n",
    "### üìä ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ:\n",
    "1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `config_optimized.json`\n",
    "2. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÑ‡∏õ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà `backend/app/config.json`\n",
    "3. Restart backend server\n",
    "4. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "### ‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:\n",
    "- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö test dataset\n",
    "- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢\n",
    "- ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏Å‡∏£‡∏ì‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "### üí° Tips:\n",
    "- ‡∏ñ‡πâ‡∏≤ F1 score ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å weights ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô 0.35, 0.30, 0.35)\n",
    "- ‡∏Ñ‡∏ß‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö cross-validation\n",
    "- ‡πÄ‡∏Å‡πá‡∏ö log ‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
