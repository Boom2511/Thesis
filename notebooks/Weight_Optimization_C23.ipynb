{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Weight Optimization for C23 Dataset\n\n## Objective:\n- Test all 3 models (Xception, F3Net, Effort-CLIP) on c23 dataset\n- Find optimal ensemble weights through grid search\n- Show comprehensive metrics (F1, Accuracy, Precision, Recall, AUC)\n- Use compute units efficiently (~10-15 units, not all 41)\n\n## Dataset Structure:\n```\ndataset_c23/\n├── manipulated_sequences/\n│   ├── Deepfakes/c23/frames/000_003/*.png\n│   ├── Face2Face/c23/frames/012_026/*.png\n│   └── FaceSwap/c23/frames/...\n└── original_sequences/\n    └── youtube/c23/frames/000/*.png\n```\n\n## test.json Format:\n```json\n[[\"original_id\", \"fake_id\"], [\"953\", \"974\"], [\"000\", \"003\"], ...]\n```\n\n**Note:** Fake video folders are named `{original_id}_{fake_id}` (e.g., \"000_003\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q timm==0.9.12 transformers==4.36.0 facenet-pytorch scikit-learn pillow==10.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Upload Dataset to Colab\n\n**Option 1: Upload ZIP to Google Drive, then extract**\n```bash\n# The dataset is already in: C:\\Users\\Admin\\Downloads\\dataset_c23.zip (21.1 GB)\n\n# Upload dataset_c23.zip to Google Drive (use Drive desktop app for large files)\n# Then in Colab:\n!unzip /content/drive/MyDrive/dataset_c23.zip -d /content/\n```\n\n**Option 2: Direct upload (NOT recommended for 21GB)**\n```python\nfrom google.colab import files\nuploaded = files.upload()  # Too slow for 21GB!\n!unzip dataset_c23.zip -d /content/\n```\n\n**Recommended:** Upload `dataset_c23.zip` to Google Drive first, then extract in Colab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set dataset path (adjust based on your upload method)\n# Note: Update this to match your extracted folder name\nDATASET_ROOT = \"/content/dataset_c23\"  # or \"/content/drive/MyDrive/dataset_c23\"\n\n# Verify structure\n!ls -la {DATASET_ROOT}\n!ls {DATASET_ROOT}/manipulated_sequences/\n!ls {DATASET_ROOT}/original_sequences/"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Model Weights from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your model weights in Google Drive\n",
    "WEIGHTS_DIR = \"/content/drive/MyDrive/deepfake-detection/backend/app/models/weights\"\n",
    "\n",
    "XCEPTION_PATH = f\"{WEIGHTS_DIR}/xception_best.pth\"\n",
    "F3NET_PATH = f\"{WEIGHTS_DIR}/f3net_best.pth\"\n",
    "EFFORT_PATH = f\"{WEIGHTS_DIR}/effort_clip_L14_trainOn_FaceForensic.pth\"\n",
    "\n",
    "# Verify files exist\n",
    "import os\n",
    "print(f\"Xception exists: {os.path.exists(XCEPTION_PATH)}\")\n",
    "print(f\"F3Net exists: {os.path.exists(F3NET_PATH)}\")\n",
    "print(f\"Effort exists: {os.path.exists(EFFORT_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Corrected Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from typing import Tuple\n",
    "\n",
    "# ================================\n",
    "# 1. CORRECTED XCEPTION MODEL\n",
    "# ================================\n",
    "\n",
    "class XceptionModel:\n",
    "    def __init__(self, weights_path: str, device: str = 'cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self._load_model(weights_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _load_model(self, weights_path: str):\n",
    "        print(\"\\n[INFO] Loading Xception model...\")\n",
    "        \n",
    "        # Create timm Xception with fc classifier (NOT last_linear)\n",
    "        model = timm.create_model('xception', pretrained=False, num_classes=2)\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "        \n",
    "        # Map keys: backbone.fc.* → fc.*\n",
    "        new_state_dict = {}\n",
    "        for k, v in checkpoint.items():\n",
    "            new_k = k.replace('module.', '')\n",
    "            new_k = new_k.replace('backbone.', '')  # CRITICAL: remove backbone prefix\n",
    "            new_k = new_k.replace('model.', '')\n",
    "            new_k = new_k.replace('encoder.', '')\n",
    "            \n",
    "            # Map last_linear to fc (timm Xception uses fc)\n",
    "            new_k = new_k.replace('last_linear.', 'fc.')\n",
    "            \n",
    "            new_state_dict[new_k] = v\n",
    "        \n",
    "        # Load weights\n",
    "        missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        # Verify classifier loaded\n",
    "        classifier_loaded = any('fc.' in k for k in new_state_dict.keys())\n",
    "        print(f\"[DEBUG] Mapped {len(new_state_dict)} keys\")\n",
    "        print(f\"[DEBUG] Classifier layer found: {classifier_loaded}\")\n",
    "        \n",
    "        if not classifier_loaded:\n",
    "            print(\"[WARNING] Classifier weights NOT found in checkpoint!\")\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        print(\"[OK] Xception model loaded\\n\")\n",
    "        return model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, image_tensor: torch.Tensor) -> Tuple[float, float]:\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        logits = self.model(image_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        real_prob = probs[0][0].item()\n",
    "        fake_prob = probs[0][1].item()\n",
    "        \n",
    "        return fake_prob, real_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 2. CORRECTED F3NET MODEL\n",
    "# ================================\n",
    "\n",
    "class F3NetModel:\n",
    "    def __init__(self, weights_path: str, device: str = 'cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self._load_model(weights_path)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _load_model(self, weights_path: str):\n",
    "        print(\"\\n[INFO] Loading F3Net model...\")\n",
    "        \n",
    "        # Create timm Xception\n",
    "        model = timm.create_model('xception', pretrained=False, num_classes=2)\n",
    "        \n",
    "        # Modify first conv for 12 channels (RGB + frequency domain)\n",
    "        original_conv1 = model.conv1\n",
    "        model.conv1 = nn.Conv2d(\n",
    "            in_channels=12,  # 3 RGB + 9 frequency channels\n",
    "            out_channels=original_conv1.out_channels,\n",
    "            kernel_size=original_conv1.kernel_size,\n",
    "            stride=original_conv1.stride,\n",
    "            padding=original_conv1.padding,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(weights_path, map_location='cpu')\n",
    "        \n",
    "        # Map keys\n",
    "        new_state_dict = {}\n",
    "        fad_head_skipped = 0\n",
    "        \n",
    "        for k, v in checkpoint.items():\n",
    "            # Skip FAD_head layers (frequency domain head - not needed)\n",
    "            if k.startswith('FAD_head'):\n",
    "                fad_head_skipped += 1\n",
    "                continue\n",
    "            \n",
    "            new_k = k.replace('module.', '')\n",
    "            new_k = new_k.replace('backbone.', '')  # CRITICAL\n",
    "            new_k = new_k.replace('model.', '')\n",
    "            new_k = new_k.replace('encoder.', '')\n",
    "            \n",
    "            # Map Sequential layer to Linear: last_linear.1.weight → fc.weight\n",
    "            new_k = new_k.replace('last_linear.1.', 'fc.')\n",
    "            new_k = new_k.replace('last_linear.', 'fc.')\n",
    "            \n",
    "            # Map other classifier names\n",
    "            new_k = new_k.replace('classifier.', 'fc.')\n",
    "            new_k = new_k.replace('head.', 'fc.')\n",
    "            \n",
    "            new_state_dict[new_k] = v\n",
    "        \n",
    "        # Load weights\n",
    "        missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        # Verify classifier loaded\n",
    "        classifier_loaded = any('fc.' in k for k in new_state_dict.keys())\n",
    "        print(f\"[DEBUG] Mapped {len(new_state_dict)} keys\")\n",
    "        print(f\"[DEBUG] Skipped {fad_head_skipped} FAD_head layers\")\n",
    "        print(f\"[DEBUG] Classifier layer found: {classifier_loaded}\")\n",
    "        \n",
    "        if not classifier_loaded:\n",
    "            print(\"[WARNING] Classifier weights NOT found in checkpoint!\")\n",
    "        \n",
    "        model = model.to(self.device)\n",
    "        print(\"[OK] F3Net model loaded\\n\")\n",
    "        return model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, image_tensor: torch.Tensor) -> Tuple[float, float]:\n",
    "        image_tensor = image_tensor.to(self.device)\n",
    "        \n",
    "        # Duplicate RGB to 12 channels (simple approach)\n",
    "        if image_tensor.shape[1] == 3:\n",
    "            image_tensor = image_tensor.repeat(1, 4, 1, 1)  # [B, 3, H, W] → [B, 12, H, W]\n",
    "        \n",
    "        logits = self.model(image_tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        real_prob = probs[0][0].item()\n",
    "        fake_prob = probs[0][1].item()\n",
    "        \n",
    "        return fake_prob, real_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================\n# 3. CORRECTED EFFORT-CLIP MODEL\n# ================================\n\nclass EffortModel:\n    def __init__(self, weights_path: str, device: str = 'cuda'):\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        self.model, self.classifier = self._load_model(weights_path)\n        self.model.eval()\n        self.classifier.eval()\n\n    def _load_model(self, weights_path: str):\n        print(\"\\n[INFO] Loading Effort-CLIP model...\")\n        \n        # Import transformers CLIP model\n        from transformers import CLIPVisionModel, CLIPVisionConfig\n        \n        # Load checkpoint\n        checkpoint = torch.load(weights_path, map_location='cpu')\n        \n        # Detect classifier input dimension\n        classifier_weight = checkpoint.get('module.head.weight', checkpoint.get('head.weight'))\n        if classifier_weight is None:\n            raise ValueError(\"Cannot find head.weight in checkpoint!\")\n        \n        hidden_dim = classifier_weight.shape[1]  # Should be 1024\n        print(f\"[DEBUG] Detected classifier input dim: {hidden_dim}\")\n        \n        # Create CLIP vision config (1024 dim, 24 layers - CLIP-L/14)\n        config = CLIPVisionConfig(\n            hidden_size=hidden_dim,\n            num_hidden_layers=24,\n            num_attention_heads=16,\n            intermediate_size=4096,\n            image_size=224,\n            patch_size=14,\n            num_channels=3\n        )\n        \n        model = CLIPVisionModel(config)\n        \n        # Map checkpoint keys to CLIP model keys\n        clip_state_dict = {}\n        loaded_count = 0\n        skipped_count = 0\n        \n        for k, v in checkpoint.items():\n            if not k.startswith('module.backbone.'):\n                continue\n            \n            # Remove prefix\n            new_k = k.replace('module.backbone.', '')\n            \n            # Skip LoRA/residual weights (S_residual, U_residual, V_residual)\n            if 'residual' in new_k.lower():\n                skipped_count += 1\n                continue\n            \n            # CRITICAL: Add vision_model. prefix for CLIPVisionModel\n            new_k = 'vision_model.' + new_k\n            \n            # Map checkpoint naming to transformers CLIP naming\n            # Remove _main suffix if exists\n            new_k = new_k.replace('.weight_main', '.weight')\n            new_k = new_k.replace('.bias_main', '.bias')\n            \n            clip_state_dict[new_k] = v\n            loaded_count += 1\n        \n        print(f\"[DEBUG] Processed {loaded_count} backbone params\")\n        print(f\"[DEBUG] Skipped {skipped_count} LoRA/residual params\")\n        \n        # Load weights into CLIP model\n        missing, unexpected = model.load_state_dict(clip_state_dict, strict=False)\n        \n        # Calculate match rate\n        total_params = len(model.state_dict())\n        loaded_params = total_params - len(missing)\n        match_rate = (loaded_params / total_params) * 100\n        \n        print(f\"[DEBUG] Loaded {loaded_params}/{total_params} params ({match_rate:.1f}% match rate)\")\n        print(f\"[WARNING] Missing keys: {len(missing)}\")\n        print(f\"[WARNING] Unexpected keys: {len(unexpected)}\")\n        \n        if match_rate < 50:\n            print(f\"\\n[ERROR] Low match rate! Model may not work correctly.\")\n            print(f\"[INFO] Sample missing keys: {list(missing)[:5]}\")\n            print(f\"[INFO] Sample unexpected keys: {list(unexpected)[:5]}\")\n        else:\n            print(f\"\\n[SUCCESS] Good match rate! Model should work correctly.\")\n        \n        model = model.to(self.device)\n        \n        # Load classifier head\n        classifier = nn.Linear(hidden_dim, 2)\n        \n        # Get head weights\n        head_weight_key = 'module.head.weight' if 'module.head.weight' in checkpoint else 'head.weight'\n        head_bias_key = 'module.head.bias' if 'module.head.bias' in checkpoint else 'head.bias'\n        \n        classifier.weight.data.copy_(checkpoint[head_weight_key])\n        classifier.bias.data.copy_(checkpoint[head_bias_key])\n        classifier = classifier.to(self.device)\n        \n        print(f\"[DEBUG] Classifier head loaded ({hidden_dim} → 2)\")\n        print(\"[OK] Effort-CLIP model loaded\\n\")\n        \n        return model, classifier\n\n    @torch.no_grad()\n    def predict(self, image_tensor: torch.Tensor) -> Tuple[float, float]:\n        image_tensor = image_tensor.to(self.device)\n        \n        # CLIP vision encoder forward pass\n        outputs = self.model(pixel_values=image_tensor)\n        features = outputs.pooler_output  # Use pooler output instead of last_hidden_state\n        features = features.to(self.device)\n        \n        # Classifier\n        logits = self.classifier(features)\n        probs = torch.softmax(logits, dim=1)\n        \n        real_prob = probs[0][0].item()\n        fake_prob = probs[0][1].item()\n        \n        return fake_prob, real_prob"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 3 models\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xception = XceptionModel(XCEPTION_PATH)\n",
    "f3net = F3NetModel(F3NET_PATH)\n",
    "effort = EffortModel(EFFORT_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load Dataset and Sample Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport random\nfrom pathlib import Path\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Load test.json\ntest_json_path = Path(DATASET_ROOT) / \"test.json\"\nwith open(test_json_path, 'r') as f:\n    test_pairs = json.load(f)\n\nprint(f\"Total test pairs: {len(test_pairs)}\")\nprint(f\"Example pairs: {test_pairs[:3]}\")\n\n# Define transforms for different models\ntransform_xception = transforms.Compose([\n    transforms.Resize((299, 299)),  # Xception input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Effort-CLIP: Try ImageNet normalization (CLIP/ViT standard)\ntransform_effort = transforms.Compose([\n    transforms.Resize((224, 224)),  # ViT/CLIP input size\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n])\n\n# Function to load image from video folder\ndef load_frame(video_id: str, is_fake: bool, method: str = \"Deepfakes\", original_id: str = None) -> Image.Image:\n    \"\"\"\n    Load a frame from the dataset.\n\n    Args:\n        video_id: Video ID (e.g., \"000\" for original, \"003\" for fake)\n        is_fake: True if loading fake video, False if loading original\n        method: Manipulation method (Deepfakes, Face2Face, FaceSwap)\n        original_id: Original video ID (needed for fake videos - folder name is original_fake)\n    \"\"\"\n    if is_fake:\n        # Fake videos are stored as \"original_fake\" (e.g., \"000_003\")\n        if original_id is None:\n            raise ValueError(\"original_id required for fake videos\")\n        folder_name = f\"{original_id}_{video_id}\"\n        frames_dir = Path(DATASET_ROOT) / \"manipulated_sequences\" / method / \"c23\" / \"frames\" / folder_name\n    else:\n        # Original videos are stored by ID (e.g., \"000\")\n        frames_dir = Path(DATASET_ROOT) / \"original_sequences\" / \"youtube\" / \"c23\" / \"frames\" / video_id\n\n    # Get first available frame\n    frame_files = sorted(frames_dir.glob(\"*.png\"))\n    if not frame_files:\n        raise FileNotFoundError(f\"No frames found in {frames_dir}\")\n\n    # Load first frame\n    return Image.open(frame_files[0]).convert('RGB')\n\n# Sample test set efficiently (to conserve compute units)\nSAMPLE_SIZE = 1000  # Sample 1000 pairs = 2000 images\nrandom.seed(42)\nsampled_pairs = random.sample(test_pairs, min(SAMPLE_SIZE, len(test_pairs)))\n\nprint(f\"\\nSampled {len(sampled_pairs)} pairs for testing\")\nprint(f\"Total images to process: {len(sampled_pairs) * 2}\")\nprint(f\"⚠️  NOTE: If only 70 pairs available, will use all of them\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare dataset\nfrom tqdm import tqdm\n\ntest_data = []\nmanipulation_methods = [\"Deepfakes\", \"Face2Face\", \"FaceSwap\"]\n\nprint(\"\\nLoading test images...\")\nerrors = 0\n\nfor original_id, fake_id in tqdm(sampled_pairs, desc=\"Loading images\"):\n    try:\n        # Load original (REAL)\n        original_img = load_frame(original_id, is_fake=False)\n        test_data.append({\n            'image_299': transform_xception(original_img),  # For Xception/F3Net\n            'image_224': transform_effort(original_img),     # For Effort-CLIP\n            'label': 0,  # 0 = REAL\n            'video_id': original_id\n        })\n        \n        # Load fake - try different methods until one works\n        # Folder name format: original_fake (e.g., \"953_974\")\n        fake_loaded = False\n        for method in manipulation_methods:\n            try:\n                fake_img = load_frame(fake_id, is_fake=True, method=method, original_id=original_id)\n                test_data.append({\n                    'image_299': transform_xception(fake_img),\n                    'image_224': transform_effort(fake_img),\n                    'label': 1,  # 1 = FAKE\n                    'video_id': f\"{original_id}_{fake_id}_{method}\"\n                })\n                fake_loaded = True\n                break\n            except FileNotFoundError:\n                continue\n        \n        if not fake_loaded:\n            errors += 1\n            \n    except Exception as e:\n        errors += 1\n        continue\n\nprint(f\"\\nLoaded {len(test_data)} images successfully\")\nprint(f\"Errors: {errors}\")\nprint(f\"Real images: {sum(1 for d in test_data if d['label'] == 0)}\")\nprint(f\"Fake images: {sum(1 for d in test_data if d['label'] == 1)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Individual Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\ndef evaluate_model(model, test_data, model_name: str, image_size: int = 299):\n    \"\"\"\n    Evaluate a single model on test data.\n    \n    Args:\n        model: The model to evaluate\n        test_data: List of test samples\n        model_name: Name for display\n        image_size: Image size to use (299 for Xception/F3Net, 224 for Effort)\n    \n    Returns:\n        predictions: List of fake probabilities\n        labels: List of ground truth labels\n        metrics: Dict of metrics\n    \"\"\"\n    predictions = []\n    labels = []\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Evaluating {model_name}\")\n    print(f\"{'='*60}\")\n    \n    # Select correct image size key\n    image_key = f'image_{image_size}'\n    \n    for item in tqdm(test_data, desc=f\"{model_name} inference\"):\n        image_tensor = item[image_key].unsqueeze(0)  # Add batch dimension\n        label = item['label']\n        \n        # Predict\n        fake_prob, real_prob = model.predict(image_tensor)\n        \n        predictions.append(fake_prob)\n        labels.append(label)\n    \n    # Convert to numpy\n    predictions = np.array(predictions)\n    labels = np.array(labels)\n    \n    # Calculate binary predictions (threshold = 0.5)\n    binary_preds = (predictions > 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(labels, binary_preds),\n        'precision': precision_score(labels, binary_preds, zero_division=0),\n        'recall': recall_score(labels, binary_preds, zero_division=0),\n        'f1': f1_score(labels, binary_preds, zero_division=0),\n        'auc': roc_auc_score(labels, predictions)\n    }\n    \n    # Print metrics\n    print(f\"\\nResults:\")\n    print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n    print(f\"  Precision: {metrics['precision']:.4f}\")\n    print(f\"  Recall:    {metrics['recall']:.4f}\")\n    print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n    print(f\"  AUC:       {metrics['auc']:.4f}\")\n    \n    return predictions, labels, metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate all 3 models (use correct image size for each)\nxception_preds, labels, xception_metrics = evaluate_model(xception, test_data, \"Xception\", image_size=299)\nf3net_preds, _, f3net_metrics = evaluate_model(f3net, test_data, \"F3Net\", image_size=299)\neffort_preds, _, effort_metrics = evaluate_model(effort, test_data, \"Effort-CLIP\", image_size=224)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Grid Search for Optimal Ensemble Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from itertools import product\n\n# Check if Effort-CLIP is working (AUC > 0.6)\neffort_working = effort_metrics['auc'] > 0.6\n\nif not effort_working:\n    print(\"⚠️  WARNING: Effort-CLIP appears to be random guessing (AUC < 0.6)\")\n    print(\"⚠️  Proceeding with 2-model ensemble (Xception + F3Net only)\\n\")\n    \n    # 2-model optimization\n    weight_range = np.arange(0.0, 1.1, 0.1)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"GRID SEARCH FOR OPTIMAL ENSEMBLE WEIGHTS (2 MODELS)\")\n    print(f\"{'='*60}\\n\")\n    \n    best_f1 = 0\n    best_weights = None\n    best_metrics = None\n    results = []\n    \n    total_combinations = 0\n    for w_xception in weight_range:\n        w_f3net = round(1.0 - w_xception, 1)\n        \n        if w_f3net < 0 or w_f3net > 1:\n            continue\n        \n        total_combinations += 1\n        \n        # Ensemble predictions (2 models only)\n        ensemble_preds = (\n            w_xception * xception_preds +\n            w_f3net * f3net_preds\n        )\n        \n        # Binary predictions\n        binary_preds = (ensemble_preds > 0.5).astype(int)\n        \n        # Calculate metrics\n        f1 = f1_score(labels, binary_preds, zero_division=0)\n        accuracy = accuracy_score(labels, binary_preds)\n        precision = precision_score(labels, binary_preds, zero_division=0)\n        recall = recall_score(labels, binary_preds, zero_division=0)\n        auc = roc_auc_score(labels, ensemble_preds)\n        \n        results.append({\n            'w_xception': w_xception,\n            'w_f3net': w_f3net,\n            'w_effort': 0.0,\n            'f1': f1,\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'auc': auc\n        })\n        \n        # Update best\n        if f1 > best_f1:\n            best_f1 = f1\n            best_weights = (w_xception, w_f3net, 0.0)\n            best_metrics = {\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'auc': auc\n            }\n\nelse:\n    print(\"✅ All 3 models working well - optimizing 3-model ensemble\\n\")\n    \n    # 3-model optimization\n    weight_range = np.arange(0.0, 1.1, 0.1)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"GRID SEARCH FOR OPTIMAL ENSEMBLE WEIGHTS (3 MODELS)\")\n    print(f\"{'='*60}\\n\")\n    \n    best_f1 = 0\n    best_weights = None\n    best_metrics = None\n    results = []\n    \n    total_combinations = 0\n    for w_xception in weight_range:\n        for w_f3net in weight_range:\n            for w_effort in weight_range:\n                # Weights must sum to 1.0 (with tolerance)\n                if abs(w_xception + w_f3net + w_effort - 1.0) > 0.01:\n                    continue\n                \n                total_combinations += 1\n                \n                # Ensemble predictions (weighted average)\n                ensemble_preds = (\n                    w_xception * xception_preds +\n                    w_f3net * f3net_preds +\n                    w_effort * effort_preds\n                )\n                \n                # Binary predictions\n                binary_preds = (ensemble_preds > 0.5).astype(int)\n                \n                # Calculate metrics\n                f1 = f1_score(labels, binary_preds, zero_division=0)\n                accuracy = accuracy_score(labels, binary_preds)\n                precision = precision_score(labels, binary_preds, zero_division=0)\n                recall = recall_score(labels, binary_preds, zero_division=0)\n                auc = roc_auc_score(labels, ensemble_preds)\n                \n                results.append({\n                    'w_xception': w_xception,\n                    'w_f3net': w_f3net,\n                    'w_effort': w_effort,\n                    'f1': f1,\n                    'accuracy': accuracy,\n                    'precision': precision,\n                    'recall': recall,\n                    'auc': auc\n                })\n                \n                # Update best\n                if f1 > best_f1:\n                    best_f1 = f1\n                    best_weights = (w_xception, w_f3net, w_effort)\n                    best_metrics = {\n                        'accuracy': accuracy,\n                        'precision': precision,\n                        'recall': recall,\n                        'f1': f1,\n                        'auc': auc\n                    }\n\nprint(f\"Tested {total_combinations} weight combinations\\n\")\nprint(f\"{'='*60}\")\nprint(\"OPTIMAL ENSEMBLE WEIGHTS FOUND\")\nprint(f\"{'='*60}\")\nprint(f\"\\nWeights:\")\nprint(f\"  Xception:    {best_weights[0]:.2f}\")\nprint(f\"  F3Net:       {best_weights[1]:.2f}\")\nprint(f\"  Effort-CLIP: {best_weights[2]:.2f}\")\nprint(f\"\\nMetrics:\")\nprint(f\"  Accuracy:  {best_metrics['accuracy']:.4f}\")\nprint(f\"  Precision: {best_metrics['precision']:.4f}\")\nprint(f\"  Recall:    {best_metrics['recall']:.4f}\")\nprint(f\"  F1 Score:  {best_metrics['f1']:.4f}\")\nprint(f\"  AUC:       {best_metrics['auc']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Optimized Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create optimized config\noptimized_config = {\n    \"models\": {\n        \"xception\": {\n            \"name\": \"xception\",\n            \"path\": \"app/models/weights/xception_best.pth\",\n            \"description\": \"Fast and reliable baseline\",\n            \"weight\": round(best_weights[0], 2),\n            \"enabled\": True\n        },\n        \"efficientnet_b4\": {\n            \"name\": \"tf_efficientnet_b4\",\n            \"path\": \"app/models/weights/effnb4_best.pth\",\n            \"description\": \"Balanced performance (DISABLED: incompatible checkpoint format)\",\n            \"weight\": 0.0,\n            \"enabled\": False\n        },\n        \"f3net\": {\n            \"name\": \"f3net\",\n            \"path\": \"app/models/weights/f3net_best.pth\",\n            \"description\": \"Frequency-aware network with spatial attention\",\n            \"weight\": round(best_weights[1], 2),\n            \"enabled\": True\n        },\n        \"effort\": {\n            \"name\": \"effort_clip\",\n            \"path\": \"app/models/weights/effort_clip_L14_trainOn_FaceForensic.pth\",\n            \"description\": \"CLIP-based multimodal detection\",\n            \"weight\": round(best_weights[2], 2),\n            \"enabled\": best_weights[2] > 0.0  # Auto-disable if weight is 0\n        }\n    },\n    \"ensemble\": {\n        \"method\": \"weighted_average\",\n        \"threshold\": 0.5,\n        \"min_models\": 2\n    },\n    \"device\": \"cuda\",\n    \"face_detection\": {\n        \"min_confidence\": 0.85,\n        \"min_face_size\": 40\n    },\n    \"inference\": {\n        \"batch_size\": 1,\n        \"generate_gradcam\": False\n    },\n    \"optimization_metadata\": {\n        \"dataset\": \"FaceForensics++ c23\",\n        \"test_samples\": len(test_data),\n        \"optimization_method\": \"grid_search\",\n        \"individual_metrics\": {\n            \"xception\": xception_metrics,\n            \"f3net\": f3net_metrics,\n            \"effort\": effort_metrics\n        },\n        \"ensemble_metrics\": best_metrics\n    }\n}\n\n# Save config\nwith open('config_optimized.json', 'w') as f:\n    json.dump(optimized_config, f, indent=2)\n\nprint(\"\\n✅ Optimized config saved to 'config_optimized.json'\")\nprint(\"\\nDownload this file and replace backend/app/config.json\")\n\n# Show configuration summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONFIGURATION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"\\nEnabled Models: {sum(1 for m in optimized_config['models'].values() if m['enabled'])}/4\")\nfor model_key, model_cfg in optimized_config['models'].items():\n    if model_cfg['enabled']:\n        print(f\"  ✅ {model_cfg['name']}: weight={model_cfg['weight']:.2f}\")\n    else:\n        print(f\"  ❌ {model_cfg['name']}: DISABLED\")\nprint(f\"\\nExpected Performance:\")\nprint(f\"  F1 Score:  {best_metrics['f1']:.2%}\")\nprint(f\"  Accuracy:  {best_metrics['accuracy']:.2%}\")\nprint(f\"  AUC:       {best_metrics['auc']:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Top 10 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Sort by F1 score\n",
    "df_results = df_results.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 ENSEMBLE CONFIGURATIONS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_results.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INDIVIDUAL MODEL PERFORMANCE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(f\"Xception:\")\n",
    "print(f\"  Accuracy: {xception_metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1:       {xception_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC:      {xception_metrics['auc']:.4f}\\n\")\n",
    "\n",
    "print(f\"F3Net:\")\n",
    "print(f\"  Accuracy: {f3net_metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1:       {f3net_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC:      {f3net_metrics['auc']:.4f}\\n\")\n",
    "\n",
    "print(f\"Effort-CLIP:\")\n",
    "print(f\"  Accuracy: {effort_metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1:       {effort_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC:      {effort_metrics['auc']:.4f}\\n\")\n",
    "\n",
    "print(f\"Ensemble (Optimized):\")\n",
    "print(f\"  Weights:  Xception={best_weights[0]:.2f}, F3Net={best_weights[1]:.2f}, Effort={best_weights[2]:.2f}\")\n",
    "print(f\"  Accuracy: {best_metrics['accuracy']:.4f}\")\n",
    "print(f\"  F1:       {best_metrics['f1']:.4f}\")\n",
    "print(f\"  AUC:      {best_metrics['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "1. **Download `config_optimized.json`** from Colab\n",
    "2. **Replace** `backend/app/config.json` with the optimized version\n",
    "3. **Restart backend server**:\n",
    "   ```bash\n",
    "   cd backend\n",
    "   python -m uvicorn app.main:app --reload\n",
    "   ```\n",
    "4. **Test** with real images to verify improvements\n",
    "\n",
    "---\n",
    "\n",
    "**Note:** If accuracy is still ~50%, it confirms that the checkpoint weights are incompatible with the dataset/architecture. In that case, you'll need to train models from scratch using Kaggle Notebooks (FREE 30hrs/week GPU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}