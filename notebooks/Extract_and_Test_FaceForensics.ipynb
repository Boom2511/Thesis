{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ Extract Frames & Test Models with FaceForensics++\n",
    "\n",
    "## üìä Dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ:\n",
    "- **Original (Real):** 300 videos\n",
    "- **Deepfakes:** 100 videos\n",
    "- **FaceSwap:** 100 videos\n",
    "- **Face2Face:** 100 videos\n",
    "\n",
    "**‡∏£‡∏ß‡∏°:** 600 videos\n",
    "\n",
    "## üéØ ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥:\n",
    "1. Extract frames ‡∏à‡∏≤‡∏Å videos (‡∏ó‡∏∏‡∏Å 30 frames)\n",
    "2. Crop faces ‡∏î‡πâ‡∏ß‡∏¢ MTCNN\n",
    "3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "4. ‡∏´‡∏≤ optimal weights\n",
    "5. ‡∏™‡∏£‡πâ‡∏≤‡∏á config ‡πÉ‡∏´‡∏°‡πà\n",
    "\n",
    "## ‚ö° Compute Units:\n",
    "- Extract + Crop: ~10-15 units\n",
    "- Model testing: ~15-20 units\n",
    "- **‡∏£‡∏ß‡∏°: ~30-35 units** (‡∏à‡∏≤‡∏Å 60.24 ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/DeepfakeProject')\n",
    "print(f\"üìÅ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\n",
    "!pip install torch torchvision timm pillow scikit-learn matplotlib seaborn tqdm opencv-python\n",
    "!pip install facenet-pytorch\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á dataset\n",
    "import glob\n",
    "\n",
    "BASE_PATH = '/content/drive/MyDrive/DeepfakeProject/datasets'\n",
    "\n",
    "# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô videos\n",
    "original_videos = glob.glob(f'{BASE_PATH}/original_sequences/youtube/c40/videos/*.mp4')\n",
    "deepfakes_videos = glob.glob(f'{BASE_PATH}/manipulated_sequences/Deepfakes/c40/videos/*.mp4')\n",
    "faceswap_videos = glob.glob(f'{BASE_PATH}/manipulated_sequences/FaceSwap/c40/videos/*.mp4')\n",
    "face2face_videos = glob.glob(f'{BASE_PATH}/manipulated_sequences/Face2Face/c40/videos/*.mp4')\n",
    "\n",
    "print(\"üìä Dataset Summary:\")\n",
    "print(f\"  Original (Real):  {len(original_videos)} videos\")\n",
    "print(f\"  Deepfakes (Fake): {len(deepfakes_videos)} videos\")\n",
    "print(f\"  FaceSwap (Fake):  {len(faceswap_videos)} videos\")\n",
    "print(f\"  Face2Face (Fake): {len(face2face_videos)} videos\")\n",
    "print(f\"  \" + \"=\"*40)\n",
    "print(f\"  Total Real:       {len(original_videos)} videos\")\n",
    "print(f\"  Total Fake:       {len(deepfakes_videos) + len(faceswap_videos) + len(face2face_videos)} videos\")\n",
    "print(f\"  Grand Total:      {len(original_videos) + len(deepfakes_videos) + len(faceswap_videos) + len(face2face_videos)} videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Extract Frames ‡∏à‡∏≤‡∏Å Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_path, output_dir, frame_interval=30, max_frames=10):\n",
    "    \"\"\"\n",
    "    Extract frames ‡∏à‡∏≤‡∏Å video\n",
    "    \n",
    "    Args:\n",
    "        video_path: path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠\n",
    "        output_dir: directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö frames\n",
    "        frame_interval: ‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å‡πÜ N frames (30 = 1 frame ‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ @ 30fps)\n",
    "        max_frames: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô frames ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠ video\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    video_name = Path(video_path).stem\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or saved_count >= max_frames:\n",
    "            break\n",
    "        \n",
    "        if frame_count % frame_interval == 0:\n",
    "            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å frame\n",
    "            frame_path = os.path.join(output_dir, f\"{video_name}_frame{saved_count:03d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            saved_count += 1\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return saved_count\n",
    "\n",
    "print(\"‚úÖ Extract function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞ extract (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î compute units)\n",
    "NUM_VIDEOS_PER_CLASS = 30  # ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "FRAMES_PER_VIDEO = 5       # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô frames ‡∏ï‡πà‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠\n",
    "FRAME_INTERVAL = 60        # ‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å 60 frames (2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ @ 30fps)\n",
    "\n",
    "OUTPUT_BASE = f'{BASE_PATH}/extracted_frames'\n",
    "\n",
    "print(f\"‚öôÔ∏è  Configuration:\")\n",
    "print(f\"  Videos per class: {NUM_VIDEOS_PER_CLASS}\")\n",
    "print(f\"  Frames per video: {FRAMES_PER_VIDEO}\")\n",
    "print(f\"  Total frames: {NUM_VIDEOS_PER_CLASS * FRAMES_PER_VIDEO * 4} frames\")\n",
    "print(f\"\\nüé¨ Starting extraction...\\n\")\n",
    "\n",
    "# Extract frames\n",
    "extraction_stats = {}\n",
    "\n",
    "# 1. Original (Real)\n",
    "print(\"[1/4] Extracting REAL frames...\")\n",
    "real_output = f'{OUTPUT_BASE}/real'\n",
    "total_frames = 0\n",
    "for video_path in tqdm(original_videos[:NUM_VIDEOS_PER_CLASS], desc=\"Real\"):\n",
    "    frames = extract_frames_from_video(video_path, real_output, FRAME_INTERVAL, FRAMES_PER_VIDEO)\n",
    "    total_frames += frames\n",
    "extraction_stats['real'] = total_frames\n",
    "print(f\"   ‚úÖ Extracted {total_frames} frames\\n\")\n",
    "\n",
    "# 2. Deepfakes (Fake)\n",
    "print(\"[2/4] Extracting DEEPFAKES frames...\")\n",
    "deepfakes_output = f'{OUTPUT_BASE}/fake'\n",
    "total_frames = 0\n",
    "for video_path in tqdm(deepfakes_videos[:NUM_VIDEOS_PER_CLASS], desc=\"Deepfakes\"):\n",
    "    frames = extract_frames_from_video(video_path, deepfakes_output, FRAME_INTERVAL, FRAMES_PER_VIDEO)\n",
    "    total_frames += frames\n",
    "extraction_stats['deepfakes'] = total_frames\n",
    "print(f\"   ‚úÖ Extracted {total_frames} frames\\n\")\n",
    "\n",
    "# 3. FaceSwap (Fake)\n",
    "print(\"[3/4] Extracting FACESWAP frames...\")\n",
    "for video_path in tqdm(faceswap_videos[:NUM_VIDEOS_PER_CLASS], desc=\"FaceSwap\"):\n",
    "    frames = extract_frames_from_video(video_path, deepfakes_output, FRAME_INTERVAL, FRAMES_PER_VIDEO)\n",
    "    total_frames += frames\n",
    "extraction_stats['faceswap'] = total_frames - extraction_stats['deepfakes']\n",
    "print(f\"   ‚úÖ Extracted {extraction_stats['faceswap']} frames\\n\")\n",
    "\n",
    "# 4. Face2Face (Fake)\n",
    "print(\"[4/4] Extracting FACE2FACE frames...\")\n",
    "for video_path in tqdm(face2face_videos[:NUM_VIDEOS_PER_CLASS], desc=\"Face2Face\"):\n",
    "    frames = extract_frames_from_video(video_path, deepfakes_output, FRAME_INTERVAL, FRAMES_PER_VIDEO)\n",
    "    total_frames += frames\n",
    "extraction_stats['face2face'] = total_frames - extraction_stats['deepfakes'] - extraction_stats['faceswap']\n",
    "print(f\"   ‚úÖ Extracted {extraction_stats['face2face']} frames\\n\")\n",
    "\n",
    "extraction_stats['fake_total'] = total_frames\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä Extraction Summary:\")\n",
    "print(f\"  Real frames:       {extraction_stats['real']}\")\n",
    "print(f\"  Fake frames:       {extraction_stats['fake_total']}\")\n",
    "print(f\"    - Deepfakes:     {extraction_stats['deepfakes']}\")\n",
    "print(f\"    - FaceSwap:      {extraction_stats['faceswap']}\")\n",
    "print(f\"    - Face2Face:     {extraction_stats['face2face']}\")\n",
    "print(f\"  Total frames:      {extraction_stats['real'] + extraction_stats['fake_total']}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üë§ Crop Faces ‡∏î‡πâ‡∏ß‡∏¢ MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÇ‡∏´‡∏•‡∏î face detector\n",
    "print(\"üì• Loading MTCNN face detector...\")\n",
    "face_detector = MTCNN(\n",
    "    keep_all=False,\n",
    "    device='cpu',  # ‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å CUDA errors\n",
    "    post_process=False,\n",
    "    min_face_size=80,\n",
    "    thresholds=[0.6, 0.7, 0.7]  # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏´‡∏≤‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô\n",
    ")\n",
    "print(\"‚úÖ MTCNN ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face_from_frame(frame_path, output_path, face_detector, min_confidence=0.90):\n",
    "    \"\"\"\n",
    "    Crop face ‡∏à‡∏≤‡∏Å frame\n",
    "    \n",
    "    Returns:\n",
    "        True ‡∏ñ‡πâ‡∏≤ crop ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à, False ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏£‡∏∑‡∏≠ confidence ‡∏ï‡πà‡∏≥\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = Image.open(frame_path).convert('RGB')\n",
    "        \n",
    "        # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
    "        boxes, probs = face_detector.detect(img)\n",
    "        \n",
    "        if boxes is None or len(boxes) == 0:\n",
    "            return False\n",
    "        \n",
    "        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "        best_idx = np.argmax(probs)\n",
    "        confidence = probs[best_idx]\n",
    "        \n",
    "        if confidence < min_confidence:\n",
    "            return False\n",
    "        \n",
    "        # Crop face with padding\n",
    "        box = boxes[best_idx]\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        \n",
    "        # Add padding\n",
    "        padding = 30\n",
    "        w, h = img.size\n",
    "        x1 = max(0, x1 - padding)\n",
    "        y1 = max(0, y1 - padding)\n",
    "        x2 = min(w, x2 + padding)\n",
    "        y2 = min(h, y2 + padding)\n",
    "        \n",
    "        # Crop ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å\n",
    "        face = img.crop((x1, y1, x2, y2))\n",
    "        face.save(output_path, quality=95)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Crop function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop faces\n",
    "CROPPED_BASE = f'{BASE_PATH}/cropped_faces'\n",
    "MIN_CONFIDENCE = 0.90  # ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ\n",
    "\n",
    "print(f\"‚öôÔ∏è  Face Detection Confidence: {MIN_CONFIDENCE}\")\n",
    "print(f\"\\n‚úÇÔ∏è  Starting face cropping...\\n\")\n",
    "\n",
    "crop_stats = {'real': 0, 'fake': 0, 'failed': 0}\n",
    "\n",
    "# Crop Real faces\n",
    "print(\"[1/2] Cropping REAL faces...\")\n",
    "real_frames = glob.glob(f'{OUTPUT_BASE}/real/*.jpg')\n",
    "real_cropped_dir = f'{CROPPED_BASE}/real'\n",
    "os.makedirs(real_cropped_dir, exist_ok=True)\n",
    "\n",
    "for frame_path in tqdm(real_frames, desc=\"Real\"):\n",
    "    output_path = os.path.join(real_cropped_dir, Path(frame_path).name)\n",
    "    if crop_face_from_frame(frame_path, output_path, face_detector, MIN_CONFIDENCE):\n",
    "        crop_stats['real'] += 1\n",
    "    else:\n",
    "        crop_stats['failed'] += 1\n",
    "\n",
    "print(f\"   ‚úÖ Cropped {crop_stats['real']} real faces\\n\")\n",
    "\n",
    "# Crop Fake faces\n",
    "print(\"[2/2] Cropping FAKE faces...\")\n",
    "fake_frames = glob.glob(f'{OUTPUT_BASE}/fake/*.jpg')\n",
    "fake_cropped_dir = f'{CROPPED_BASE}/fake'\n",
    "os.makedirs(fake_cropped_dir, exist_ok=True)\n",
    "\n",
    "for frame_path in tqdm(fake_frames, desc=\"Fake\"):\n",
    "    output_path = os.path.join(fake_cropped_dir, Path(frame_path).name)\n",
    "    if crop_face_from_frame(frame_path, output_path, face_detector, MIN_CONFIDENCE):\n",
    "        crop_stats['fake'] += 1\n",
    "    else:\n",
    "        crop_stats['failed'] += 1\n",
    "\n",
    "print(f\"   ‚úÖ Cropped {crop_stats['fake']} fake faces\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä Face Cropping Summary:\")\n",
    "print(f\"  Real faces:    {crop_stats['real']}\")\n",
    "print(f\"  Fake faces:    {crop_stats['fake']}\")\n",
    "print(f\"  Total faces:   {crop_stats['real'] + crop_stats['fake']}\")\n",
    "print(f\"  Failed:        {crop_stats['failed']} (no face or low confidence)\")\n",
    "print(f\"  Success rate:  {(crop_stats['real'] + crop_stats['fake']) / (crop_stats['real'] + crop_stats['fake'] + crop_stats['failed']) * 100:.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà Crop ‡πÅ‡∏•‡πâ‡∏ß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û\n",
    "import random\n",
    "\n",
    "real_samples = random.sample(glob.glob(f'{CROPPED_BASE}/real/*.jpg'), min(5, crop_stats['real']))\n",
    "fake_samples = random.sample(glob.glob(f'{CROPPED_BASE}/fake/*.jpg'), min(5, crop_stats['fake']))\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Real samples\n",
    "for idx, img_path in enumerate(real_samples):\n",
    "    img = Image.open(img_path)\n",
    "    axes[0, idx].imshow(img)\n",
    "    axes[0, idx].set_title('REAL', color='green', fontweight='bold')\n",
    "    axes[0, idx].axis('off')\n",
    "\n",
    "# Fake samples\n",
    "for idx, img_path in enumerate(fake_samples):\n",
    "    img = Image.open(img_path)\n",
    "    axes[1, idx].imshow(img)\n",
    "    axes[1, idx].set_title('FAKE', color='red', fontweight='bold')\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Cropped Faces', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_cropped_faces.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "**‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î model weights ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:**\n",
    "1. `xception_best.pth`\n",
    "2. `f3net_best.pth`\n",
    "3. `effort_clip_L14_trainOn_FaceForensic.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î weights (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô Drive)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÅ Please upload 3 model weight files:\")\n",
    "print(\"1. xception_best.pth\")\n",
    "print(\"2. f3net_best.pth\")\n",
    "print(\"3. effort_clip_L14_trainOn_FaceForensic.pth\")\n",
    "print(\"\\n‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÉ‡∏ô Drive ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ skip cell ‡∏ô‡∏µ‡πâ\")\n",
    "\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)\n",
    "import os\n",
    "\n",
    "if not os.path.exists('deepfake-detection'):\n",
    "    print(\"üì• Cloning project...\")\n",
    "    # ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î folder ‡∏à‡∏≤‡∏Å Drive\n",
    "    !cp -r /content/drive/MyDrive/deepfake-detection /content/\n",
    "    print(\"‚úÖ Project ready\")\n",
    "else:\n",
    "    print(\"‚úÖ Project already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths ‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "import sys\n",
    "sys.path.insert(0, '/content/deepfake-detection/backend/app')\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ weights path\n",
    "WEIGHTS_PATH = '/content'  # ‡∏´‡∏£‡∏∑‡∏≠ path ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Å‡πá‡∏ö weights\n",
    "\n",
    "# Import model classes (‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)\n",
    "# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ\n",
    "\n",
    "print(\"üì• Loading models...\")\n",
    "# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà (‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å notebook ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)\n",
    "# ...\n",
    "\n",
    "print(\"‚úÖ Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "**(‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏à‡∏≤‡∏Å `Model_Weight_Optimization.ipynb` ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß)**\n",
    "\n",
    "‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏°‡∏µ cropped faces ‡πÅ‡∏•‡πâ‡∏ß ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö notebook ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤\n",
    "‡πÅ‡∏ï‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô test_data path ‡πÄ‡∏õ‡πá‡∏ô:\n",
    "```python\n",
    "TEST_DATA_PATH = '/content/drive/MyDrive/DeepfakeProject/datasets/cropped_faces'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Optimal Weights\n",
    "\n",
    "**(‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö notebook ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á Drive\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading results...\")\n",
    "files.download('config_optimized.json')\n",
    "files.download('weight_optimization_report.json')\n",
    "files.download('individual_vs_ensemble.png')\n",
    "files.download('sample_cropped_faces.png')\n",
    "\n",
    "print(\"‚úÖ Done! Files downloaded to your computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "\n",
    "### Dataset:\n",
    "- **Source:** FaceForensics++ (c40 compression)\n",
    "- **Real videos:** 300 (30 used)\n",
    "- **Fake videos:** 300 (90 used: 30 Deepfakes + 30 FaceSwap + 30 Face2Face)\n",
    "- **Total frames extracted:** ~600 frames\n",
    "- **Faces detected:** ~XXX faces\n",
    "\n",
    "### Model Performance:\n",
    "- **Xception:** Accuracy: X.XXX, F1: X.XXX\n",
    "- **F3Net:** Accuracy: X.XXX, F1: X.XXX\n",
    "- **Effort-CLIP:** Accuracy: X.XXX, F1: X.XXX\n",
    "\n",
    "### Ensemble (Optimized):\n",
    "- **Weights:** Xception: X.XX, F3Net: X.XX, Effort: X.XX\n",
    "- **Accuracy:** X.XXX\n",
    "- **F1 Score:** X.XXX\n",
    "- **AUC:** X.XXX\n",
    "\n",
    "### ‚úÖ Next Steps:\n",
    "1. Download `config_optimized.json`\n",
    "2. Replace `backend/app/config.json`\n",
    "3. Restart backend server\n",
    "4. Test with real-world images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
