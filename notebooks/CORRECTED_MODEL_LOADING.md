# ðŸ”§ à¹à¸à¹‰à¹„à¸‚ Model Loading - à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ (3 à¹‚à¸¡à¹€à¸”à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”!)

## ðŸ“‹ à¸ªà¸£à¸¸à¸›à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¸žà¸š:

à¸ˆà¸²à¸ checkpoint inspection (`output.md`):

### **Xception** (`xception_best.pth`):
```
âœ… Structure: Dict with keys directly
âœ… Has classifier: backbone.last_linear.weight/bias
âŒ Problem: "backbone." prefix à¸•à¹‰à¸­à¸‡à¸¥à¸šà¸­à¸­à¸
```

### **F3Net** (`f3net_best.pth`):
```
âœ… Structure: Dict with keys directly
âœ… Has classifier: backbone.last_linear.1.weight/bias (Sequential layer!)
âœ… Has FAD_head: FAD_head.layer1.weight (frequency analysis)
âŒ Problem 1: "backbone." prefix à¸•à¹‰à¸­à¸‡à¸¥à¸šà¸­à¸­à¸
âŒ Problem 2: "last_linear.1." â†’ "last_linear." (Sequential â†’ Linear)
âŒ Problem 3: FAD_head à¸•à¹‰à¸­à¸‡ skip (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰à¹ƒà¸™à¹‚à¸¡à¹€à¸”à¸¥ timm)
```

### **Effort-CLIP** (`effort_clip_L14_trainOn_FaceForensic.pth`):
```
âœ… Structure: Dict with keys directly
âœ… Has CLIP encoder: module.backbone.* (ViT 1024 dim, 24 layers)
âœ… Has classifier: module.head.weight/bias (1024 â†’ 2)
âŒ Problem 1: "module.backbone." prefix à¸•à¹‰à¸­à¸‡à¸¥à¸šà¸­à¸­à¸
âŒ Problem 2: à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ transformers ViT (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ CLIP ViT-L/14 à¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™ 768 dim)
```

---

## âœ… à¹‚à¸„à¹‰à¸”à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸‚à¹à¸¥à¹‰à¸§ - à¸„à¸±à¸”à¸¥à¸­à¸à¹„à¸›à¹à¸—à¸™à¸—à¸µà¹ˆ Cell 11

```python
import timm
from pathlib import Path

# ========================================
# 1. Xception Model - CORRECTED
# ========================================
class XceptionModel:
    def __init__(self, weights_path: str, device: torch.device):
        self.device = device
        self.model = self._load_model(weights_path)
        self.model.eval()

    def _load_model(self, weights_path: str) -> nn.Module:
        print(f"\nðŸ”§ Loading Xception from {Path(weights_path).name}")

        # à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸¡à¹€à¸”à¸¥
        model = timm.create_model('xception', pretrained=False, num_classes=2)

        # à¹‚à¸«à¸¥à¸” checkpoint
        checkpoint = torch.load(weights_path, map_location='cpu')

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡
        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
                print("  âœ… Using checkpoint['model']")
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                print("  âœ… Using checkpoint['state_dict']")
            else:
                state_dict = checkpoint
                print("  âœ… Using checkpoint directly")
        else:
            state_dict = checkpoint
            print("  âœ… Checkpoint is state_dict")

        # âœ… FIX: à¸¥à¸š "backbone." prefix
        new_state_dict = {}
        for k, v in state_dict.items():
            # à¸¥à¸š prefix
            new_k = k.replace('module.', '')
            new_k = new_k.replace('backbone.', '')  # â† à¸ªà¸³à¸„à¸±à¸!
            new_k = new_k.replace('model.', '')
            new_k = new_k.replace('encoder.', '')

            # Map classifier layer names
            if 'fc.' in new_k:
                new_k = new_k.replace('fc.', 'last_linear.')
            elif 'classifier.' in new_k:
                new_k = new_k.replace('classifier.', 'last_linear.')
            elif 'head.' in new_k:
                new_k = new_k.replace('head.', 'last_linear.')

            new_state_dict[new_k] = v

        print(f"  ðŸ“Š Loaded {len(new_state_dict)} parameters")

        # à¹‚à¸«à¸¥à¸”à¹€à¸‚à¹‰à¸²à¹‚à¸¡à¹€à¸”à¸¥
        result = model.load_state_dict(new_state_dict, strict=False)

        if result.missing_keys:
            print(f"  âš ï¸  Missing keys: {len(result.missing_keys)}")

        if result.unexpected_keys:
            print(f"  âš ï¸  Unexpected keys: {len(result.unexpected_keys)}")

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² classifier à¹‚à¸«à¸¥à¸”à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
        classifier_loaded = any('last_linear' in k for k in new_state_dict.keys())
        if classifier_loaded:
            print("  âœ… Classifier layer loaded")
        else:
            print("  âŒ WARNING: Classifier layer NOT loaded!")

        model.to(self.device)
        return model

    @torch.no_grad()
    def predict(self, image_tensor: torch.Tensor):
        image_tensor = image_tensor.to(self.device)
        logits = self.model(image_tensor)
        probs = torch.softmax(logits, dim=1)
        real_prob = probs[0][0].item()
        fake_prob = probs[0][1].item()
        return fake_prob, real_prob


# ========================================
# 2. F3Net Model - CORRECTED
# ========================================
class F3NetModel:
    def __init__(self, weights_path: str, device: torch.device):
        self.device = device
        self.model = self._load_model(weights_path)
        self.model.eval()

    def _load_model(self, weights_path: str) -> nn.Module:
        print(f"\nðŸ”§ Loading F3Net from {Path(weights_path).name}")

        # F3Net à¹ƒà¸Šà¹‰ Xception architecture
        model = timm.create_model('xception', pretrained=False, num_classes=2)

        checkpoint = torch.load(weights_path, map_location='cpu')

        if isinstance(checkpoint, dict):
            if 'model' in checkpoint:
                state_dict = checkpoint['model']
                print("  âœ… Using checkpoint['model']")
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                print("  âœ… Using checkpoint['state_dict']")
            else:
                state_dict = checkpoint
                print("  âœ… Using checkpoint directly")
        else:
            state_dict = checkpoint
            print("  âœ… Checkpoint is state_dict")

        # âœ… FIX: à¸¥à¸š "backbone." à¹à¸¥à¸°à¹à¸›à¸¥à¸‡ Sequential layer
        new_state_dict = {}
        fad_head_skipped = 0

        for k, v in state_dict.items():
            # Skip FAD_head (frequency analysis head à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰)
            if k.startswith('FAD_head'):
                fad_head_skipped += 1
                continue

            # à¸¥à¸š prefix
            new_k = k.replace('module.', '')
            new_k = new_k.replace('backbone.', '')  # â† à¸ªà¸³à¸„à¸±à¸!
            new_k = new_k.replace('model.', '')
            new_k = new_k.replace('encoder.', '')

            # âœ… FIX: à¹à¸›à¸¥à¸‡ Sequential layer (last_linear.1) â†’ Linear (last_linear)
            new_k = new_k.replace('last_linear.1.', 'last_linear.')  # â† à¸ªà¸³à¸„à¸±à¸!

            # Map classifier layer names
            if 'fc.' in new_k:
                new_k = new_k.replace('fc.', 'last_linear.')
            elif 'classifier.' in new_k:
                new_k = new_k.replace('classifier.', 'last_linear.')
            elif 'head.' in new_k:
                new_k = new_k.replace('head.', 'last_linear.')

            new_state_dict[new_k] = v

        print(f"  ðŸ“Š Loaded {len(new_state_dict)} parameters")
        print(f"  ðŸ—‘ï¸  Skipped {fad_head_skipped} FAD_head layers")

        # à¹‚à¸«à¸¥à¸”à¹€à¸‚à¹‰à¸²à¹‚à¸¡à¹€à¸”à¸¥
        result = model.load_state_dict(new_state_dict, strict=False)

        if result.missing_keys:
            print(f"  âš ï¸  Missing keys: {len(result.missing_keys)}")

        if result.unexpected_keys:
            print(f"  âš ï¸  Unexpected keys: {len(result.unexpected_keys)}")

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸² classifier à¹‚à¸«à¸¥à¸”à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
        classifier_loaded = any('last_linear' in k for k in new_state_dict.keys())
        if classifier_loaded:
            print("  âœ… Classifier layer loaded")
        else:
            print("  âŒ WARNING: Classifier layer NOT loaded!")

        model.to(self.device)
        return model

    @torch.no_grad()
    def predict(self, image_tensor: torch.Tensor):
        image_tensor = image_tensor.to(self.device)
        logits = self.model(image_tensor)
        probs = torch.softmax(logits, dim=1)
        real_prob = probs[0][0].item()
        fake_prob = probs[0][1].item()
        return fake_prob, real_prob


# ========================================
# 3. Effort-CLIP Model - CORRECTED
# ========================================
class EffortModel:
    def __init__(self, weights_path: str, device: torch.device):
        self.device = device
        self.model, self.classifier = self._load_model(weights_path)
        self.model.eval()
        self.classifier.eval()

    def _load_model(self, weights_path: str):
        print(f"\nðŸ”§ Loading Effort-CLIP from {Path(weights_path).name}")

        checkpoint = torch.load(weights_path, map_location='cpu')

        # Import transformers ViT
        from transformers import ViTModel, ViTConfig

        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š dimension à¸ˆà¸²à¸ checkpoint
        if 'module.head.weight' in checkpoint:
            head_input_dim = checkpoint['module.head.weight'].shape[1]
            print(f"  ðŸ“Š Detected classifier input dim: {head_input_dim}")
        else:
            head_input_dim = 1024
            print(f"  âš ï¸  Using default dim: {head_input_dim}")

        # à¸ªà¸£à¹‰à¸²à¸‡ ViT config (à¸•à¸²à¸¡ checkpoint: 1024 dim, 24 layers)
        config = ViTConfig(
            hidden_size=1024,
            num_hidden_layers=24,
            num_attention_heads=16,
            intermediate_size=4096,
            image_size=224,
            patch_size=14,
            num_channels=3
        )
        model = ViTModel(config).to(self.device)

        # à¹‚à¸«à¸¥à¸” backbone weights
        backbone_state_dict = {}
        for k, v in checkpoint.items():
            if k.startswith('module.backbone.'):
                # à¸¥à¸š 'module.backbone.' prefix
                new_k = k.replace('module.backbone.', '')
                backbone_state_dict[new_k] = v

        result = model.load_state_dict(backbone_state_dict, strict=False)
        print(f"  âœ… Backbone loaded: {len(backbone_state_dict)} params")

        if result.missing_keys:
            print(f"  âš ï¸  Missing keys: {len(result.missing_keys)}")

        # à¸ªà¸£à¹‰à¸²à¸‡ classifier à¹à¸¥à¸°à¹‚à¸«à¸¥à¸” weights
        classifier = nn.Linear(head_input_dim, 2).to(self.device)

        if 'module.head.weight' in checkpoint and 'module.head.bias' in checkpoint:
            classifier.weight.data = checkpoint['module.head.weight']
            classifier.bias.data = checkpoint['module.head.bias']
            print(f"  âœ… Classifier head loaded ({head_input_dim} â†’ 2)")
        else:
            print(f"  âŒ WARNING: Classifier head NOT found in checkpoint!")

        return model, classifier

    @torch.no_grad()
    def predict(self, image_tensor: torch.Tensor):
        image_tensor = image_tensor.to(self.device)

        # ViT forward pass
        outputs = self.model(pixel_values=image_tensor)
        # Extract [CLS] token embedding
        features = outputs.last_hidden_state[:, 0, :]  # Shape: [batch, 1024]

        # Classifier forward pass
        logits = self.classifier(features)
        probs = torch.softmax(logits, dim=1)

        real_prob = probs[0][0].item()
        fake_prob = probs[0][1].item()

        return fake_prob, real_prob


print("âœ… CORRECTED model classes defined")
print("   â†’ All 3 models have classifier heads!")
```

---

## ðŸš€ à¸§à¸´à¸˜à¸µà¹ƒà¸Šà¹‰:

### **Step 1: à¹à¸à¹‰ Cell 3 (à¹€à¸žà¸´à¹ˆà¸¡ transformers)**

```python
# à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ dependencies
!pip install -q torch torchvision timm pillow scikit-learn tqdm
!pip install -q git+https://github.com/openai/CLIP.git
!pip install -q transformers  # â† à¹€à¸žà¸´à¹ˆà¸¡à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸µà¹‰!

print("âœ… Dependencies installed")
```

### **Step 2: à¹à¸—à¸™à¸—à¸µà¹ˆ Cell 11 à¸”à¹‰à¸§à¸¢à¹‚à¸„à¹‰à¸”à¸”à¹‰à¸²à¸™à¸šà¸™**

à¸„à¸±à¸”à¸¥à¸­à¸à¹‚à¸„à¹‰à¸”à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸”à¹‰à¸²à¸™à¸šà¸™à¹„à¸›à¹à¸—à¸™à¸—à¸µà¹ˆ Cell 11 à¹€à¸”à¸´à¸¡

### **Step 3: à¹ƒà¸Šà¹‰ Cell 12-19 à¸•à¸²à¸¡à¹€à¸”à¸´à¸¡ (3 à¹‚à¸¡à¹€à¸”à¸¥)**

```python
# Cell 12 - à¹‚à¸«à¸¥à¸”à¸—à¸±à¹‰à¸‡ 3 à¹‚à¸¡à¹€à¸”à¸¥
xception = XceptionModel(XCEPTION_PATH, device)
f3net = F3NetModel(F3NET_PATH, device)
effort = EffortModel(EFFORT_PATH, device)

models = {
    'xception': xception,
    'f3net': f3net,
    'effort': effort
}

print(f"\nðŸŽ¯ Total models loaded: {len(models)}")
```

### **Step 4: Cell 18-19 à¹ƒà¸Šà¹‰à¸•à¸²à¸¡à¹€à¸”à¸´à¸¡ (3 à¹‚à¸¡à¹€à¸”à¸¥ ensemble)**

```python
# Cell 18
def evaluate_ensemble(weights, results):
    """à¸›à¸£à¸°à¹€à¸¡à¸´à¸™ ensemble à¸”à¹‰à¸§à¸¢ weights à¸—à¸µà¹ˆà¸à¸³à¸«à¸™à¸”"""
    w_xception, w_f3net, w_effort = weights  # 3 weights

    ensemble_pred = (
        results['xception']['predictions'] * w_xception +
        results['f3net']['predictions'] * w_f3net +
        results['effort']['predictions'] * w_effort
    )

    labels = results['xception']['labels']
    pred_labels = (ensemble_pred > 0.5).astype(int)

    acc = accuracy_score(labels, pred_labels)
    f1 = f1_score(labels, pred_labels, zero_division=0)
    auc = roc_auc_score(labels, ensemble_pred)

    return {'accuracy': acc, 'f1': f1, 'auc': auc}
```

```python
# Cell 19 - Grid search (à¹ƒà¸Šà¹‰à¸•à¸²à¸¡à¹€à¸”à¸´à¸¡)
step = 0.05
weight_range = np.arange(0.0, 1.0 + step, step)

best_score = 0
best_weights = None
best_metrics = None
all_results = []

for w1 in tqdm(weight_range, desc="Grid Search"):
    for w2 in weight_range:
        w3 = 1.0 - w1 - w2

        if w3 < 0 or w3 > 1.0 or abs(w1 + w2 + w3 - 1.0) > 0.01:
            continue

        weights = (w1, w2, w3)
        metrics = evaluate_ensemble(weights, results)
        score = metrics['f1']

        all_results.append({
            'weights': weights,
            'metrics': metrics,
            'score': score
        })

        if score > best_score:
            best_score = score
            best_weights = weights
            best_metrics = metrics
```

### **Step 5: Cell 25 - Config file (à¹€à¸›à¸´à¸”à¹ƒà¸Šà¹‰à¸—à¸±à¹‰à¸‡ 3 à¹‚à¸¡à¹€à¸”à¸¥)**

```python
new_config = {
  "models": {
    "xception": {
      "name": "xception",
      "path": "app/models/weights/xception_best.pth",
      "description": "Fast and reliable baseline",
      "weight": round(best_weights[0], 2),
      "enabled": True
    },
    "efficientnet_b4": {
      "name": "tf_efficientnet_b4",
      "path": "app/models/weights/effnb4_best.pth",
      "description": "Balanced performance (DISABLED: incompatible checkpoint)",
      "weight": 0.0,
      "enabled": False
    },
    "f3net": {
      "name": "f3net",
      "path": "app/models/weights/f3net_best.pth",
      "description": "Frequency-aware network with spatial attention",
      "weight": round(best_weights[1], 2),
      "enabled": True
    },
    "effort": {
      "name": "effort_clip",
      "path": "app/models/weights/effort_clip_L14_trainOn_FaceForensic.pth",
      "description": "CLIP-based multimodal detection",
      "weight": round(best_weights[2], 2),
      "enabled": True  # â† à¹€à¸›à¸´à¸”à¹ƒà¸Šà¹‰à¸‡à¸²à¸™!
    }
  },
  "ensemble": {
    "method": "weighted_average",
    "threshold": 0.5,
    "min_models": 2
  },
  "device": "cuda",
  "face_detection": {
    "min_confidence": 0.85,
    "min_face_size": 40
  },
  "inference": {
    "batch_size": 1,
    "generate_gradcam": False
  }
}

with open('config_optimized.json', 'w') as f:
    json.dump(new_config, f, indent=2)

print("âœ… Config saved: config_optimized.json (3 models)")
print("\nðŸ“‹ à¸„à¸±à¸”à¸¥à¸­à¸à¹„à¸›à¹à¸—à¸™à¸—à¸µà¹ˆ: backend/app/config.json")
```

---

## ðŸŽ¯ à¸œà¸¥à¸—à¸µà¹ˆà¸„à¸²à¸”à¸«à¸§à¸±à¸‡:

### **à¸«à¸¥à¸±à¸‡ run Cell 12:**
```
ðŸ”§ Loading Xception from xception_best.pth
  âœ… Using checkpoint directly
  ðŸ“Š Loaded XXX parameters
  âœ… Classifier layer loaded

ðŸ”§ Loading F3Net from f3net_best.pth
  âœ… Using checkpoint directly
  ðŸ“Š Loaded XXX parameters
  ðŸ—‘ï¸  Skipped XX FAD_head layers
  âœ… Classifier layer loaded

ðŸ”§ Loading Effort-CLIP from effort_clip_L14_trainOn_FaceForensic.pth
  ðŸ“Š Detected classifier input dim: 1024
  âœ… Backbone loaded: XXX params
  âœ… Classifier head loaded (1024 â†’ 2)

ðŸŽ¯ Total models loaded: 3
```

### **à¸«à¸¥à¸±à¸‡ run Cell 16 (evaluation):**
```
ðŸ“Š XCEPTION Performance:
  Accuracy:  0.90-0.97 âœ…
  Precision: 0.88-0.96
  Recall:    0.89-0.95
  F1 Score:  0.89-0.96
  AUC:       0.95-0.99

ðŸ“Š F3NET Performance:
  Accuracy:  0.90-0.97 âœ…
  (similar metrics)

ðŸ“Š EFFORT Performance:
  Accuracy:  0.85-0.95 âœ…
  (similar metrics)
```

### **à¸«à¸¥à¸±à¸‡ run Cell 19 (optimal weights):**
```
ðŸ† BEST ENSEMBLE CONFIGURATION
ðŸ“Š Optimal Weights:
  Xception:    0.XXX (XX.X%)
  F3Net:       0.XXX (XX.X%)
  Effort-CLIP: 0.XXX (XX.X%)

ðŸ“ˆ Performance:
  Accuracy: 0.92-0.98 âœ…âœ…
  F1 Score: 0.91-0.97
  AUC:      0.96-0.99
```

---

## ðŸ“ Checklist:

- [ ] à¹à¸à¹‰ Cell 3 (à¹€à¸žà¸´à¹ˆà¸¡ `transformers`)
- [ ] à¹à¸—à¸™à¸—à¸µà¹ˆ Cell 11 (3 model classes à¹ƒà¸«à¸¡à¹ˆ)
- [ ] Cell 12-19 à¹ƒà¸Šà¹‰à¸•à¸²à¸¡à¹€à¸”à¸´à¸¡ (3 models)
- [ ] à¹à¸à¹‰ Cell 25 (Effort `enabled: true`)
- [ ] Runtime â†’ Restart and run all
- [ ] à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š "âœ… Classifier layer loaded" à¸—à¸±à¹‰à¸‡ 3 à¹‚à¸¡à¹€à¸”à¸¥
- [ ] à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š accuracy > 85% à¸—à¸±à¹‰à¸‡ 3 à¹‚à¸¡à¹€à¸”à¸¥
- [ ] Download `config_optimized.json`

---

**Updated:** 25 à¸•à¸¸à¸¥à¸²à¸„à¸¡ 2025
**Status:** à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ âœ… (à¸—à¸±à¹‰à¸‡ 3 à¹‚à¸¡à¹€à¸”à¸¥!)
